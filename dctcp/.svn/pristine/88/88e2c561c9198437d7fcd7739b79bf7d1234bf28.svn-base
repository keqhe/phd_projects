\subsection{Incast}
\label{incast}

\iffalse
%%% incast, 16 to 1 %%%
% see , /power/home/keq/workloads/program/acdctcp/incast-container16-10Minutes
\begin{table*}[!htb]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
  & Avg Tput (Mbps) & Fairness Index & 50$^{th}$ percentile TCP RTT ($\mu$s) & 99.9$^{th}$ percentile TCP RTT ($\mu$s) & Drop Rate \\
 \hline

 Default & 619 & 0.98 & 3607 & 11599 & 0.14\% \\
 DCTCP  & 619 & 0.99 & 197 & 486 & 0 \\
 Ours & 598 & 0.99 &  93 & 289 & 0 \\
 \hline
\end{tabular}
\caption{16 to 1 incast test. MTU = 9000 B. Marking min = 10.}
\label{incast_16to1_tbl_9000}
\end{center}
\end{table*}

%see, /power/home/keq/workloads/program/acdctcp/incast-container32-10Minutes
\begin{table*}[!htb]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
  & Avg Tput (Mbps) & Fairness Index & 50$^{th}$ percentile TCP RTT ($\mu$s) & 99.9$^{th}$ percentile TCP RTT ($\mu$s) & Drop Rate \\
 \hline

 Default & 318 & 0.97 & 3729 & 14448 & 0.37\% \\
 DCTCP  & 319 & 0.99 & 448 & 792 & 0 \\
 Ours & 303 & 0.99 &  103 & 249 & 0 \\
 \hline
\end{tabular}
\caption{32 to 1 incast test. MTU = 9000 B. Marking min = 10.}
\label{incast_32to1_tbl_9000_markingMin10}
\end{center}
\end{table*}

%see, /power/home/keq/workloads/program/acdctcp/incast-container41
\begin{table*}[!htb]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
  & Avg Tput (Mbps) & Fairness Index & 50$^{th}$ percentile TCP RTT ($\mu$s) & 99.9$^{th}$ percentile TCP RTT ($\mu$s) & Drop Rate \\
 \hline

 Default & 245 & 0.96 & 3767 & 14596 & 0.55\% \\
 DCTCP  & 247 & 0.99 & 589 & 973 & 0 \\
 Ours & 238 & 0.99 &  130 & 327 & 0 \\
 \hline
\end{tabular}
\caption{40 to 1 incast test. MTU = 9000 B. Marking min = 10.}
\label{incast_40to1_tbl_9000_markingMin10}
\end{center}
\end{table*}


%see, /power/home/keq/workloads/program/acdctcp/incast-container48
\begin{table*}[!htb]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c| }
 \hline
  & Avg Tput (Mbps) & Fairness Index & 50$^{th}$ percentile TCP RTT ($\mu$s) & 99.9$^{th}$ percentile TCP RTT ($\mu$s) & Drop Rate \\
 \hline

 Default & 210 & 0.98 & 3792 & 15776 & 0.71\% \\
 DCTCP  & 210 & 0.99 & 684 & 1000 & 0 \\
 Ours & 201 & 0.99 &  121 & 314 & 0 \\
 \hline
\end{tabular}
\caption{47 to 1 incast test. MTU = 9000 B. Marking min = 10.}
\label{incast_47to1_tbl_9000_markingMin10}
\end{center}
\end{table*}

\fi

%%%%%%%%%%using figures to present incast: tput, fariness, droprate, TCP RTT

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/incast/plots9k/incast_sockperf50th_vary_sender.pdf}
                \caption{50$^{th}$ percentile TCP RTT ($\mu$s).}
                \label{incast_9k_50th_sockperf}
        \end{subfigure}
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/incast/plots9k/incast_sockperf999th_vary_sender.pdf}
                \caption{99.9$^{th}$ percentile TCP RTT ($\mu$s).}
                \label{incast_9k_999th_sockperf}
        \end{subfigure}
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/incast/plots9k/incast_droprate_vary_sender.pdf}
                \caption{packet drop rate.}
                \label{incast_9k_droprate}
        \end{subfigure}
	\caption{Many to one incast: TCP RTT and packet drop rate.}
	\label{incast_9k_sockperf_droprate}
\end{figure*}

\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/incast/plots9k/incast_tput_vary_sender.pdf}
                \caption{average throughput.}
                \label{incast_9k_tput}
        \end{subfigure}
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/incast/plots9k/incast_fairness_vary_sender.pdf}
                \caption{fairness.}
                \label{incast_9k_fariness}
        \end{subfigure}
        \caption{Many to one incast: throughput and fairness.}
        \label{incast_9k_tput_fairness}
\end{figure}


%%%%47 to 1 incast, sockperf CDF %%%
\begin{figure}[t]
        \centering
  \includegraphics[width=0.45\textwidth]{figures/incast/47to1/incast_47to1_test_sockperf.pdf}
        \caption{TCP RTT in 47 to 1 incast test.}
        \label{sockperf_incast_47to1}
\end{figure}

\begin{figure}[t]
        \centering
  \includegraphics[width=0.45\textwidth]{figures/incast/pressure/incast_pressure_compare_sockperf.pdf}
        \caption{TCP RTT measurement results when almost all ports are congested.}
        \label{sockperf_pressure_incast}
\end{figure}


In this section, we evaluate LiquidSwitch's performance in incast scenarios. 
For each experiment (i.e., each setting), we run it for 10 minutes.

\tightparagraph{Many to one}
First we test how \acdc{} performs in many to one incast. 
We have 17 physical servers in the testbed and each server has 4 NIC interfaces. 
To utilize multiple NIC interfaces, 
we set up 3 Linux containers on each server and let each container use one interface on the server. 
In this way, we can scale the number of instances to 51. 
We connect 48 containers to the switch such that all of 48 10Gbps ports on the switch are utilized. 
Then we gradually increase of extent of incast (number of concurrent senders is 16, 32, 40 and 47). 
Figure~\ref{incast_9k_sockperf_droprate} shows TCP RTT and packet drop rate results.
Figure~\ref{sockperf_incast_47to1} shows TCP RTT CDF in 47 to 1 incast. 
When there are 47 concurrent senders, DCTCP can reduce median TCP RTT by 82\% and \acdc{} can reduce by 97\%; 
DCTCP can reduce 99.9$^{th}$ percentile TCP RTT by 94\% and \acdc{} can reduce by 98\%; 
both DCTCP and \acdc{} get 0\% packet drop rate. \acdc{}â€™s performance is better than DCTCP, 
especially when the number of senders increases, The reason is that 
Linux DCTCP code (as well as the Stanford implementation)
puts a lower bound (which is 2) on \cwnd{} value. 
In the many to one incast test, we have up to 47 concurrent competing flows and
the network's MTU size is 9KB. In this case,
the lower bound of \cwnd{} can not reduce queue occupancy further.
So DCTCP's TCP increases gradually when the number of concurrent senders increases. 
This issue was also mentioned by~\cite{judd2015nsdi}.
In our scheme, we control \rwnd{} (which is in bytes) instead of \cwnd{} (which is in packets).
\rwnd{}'s lowest value can be much smaller than 2*MSS.
So we can reduce the throughput further.
We checked that if we also put a lower bound, then our TCP RTT will be increased too.
Furthermore, controlling \rwnd{} is more fine-grained.
Figure~\ref{incast_9k_tput_fairness} shows TCP throughput and fairness results.
Both DCTCP and \acdc{} gets comparable throughput as Default and both offer a fairness index greater than 0.99\%.
We also run the same set of tests with MTU size 1500 bytes, 
we observe that DCTCP and \acdc{} almost have identical performance (TCP RTT, loss rate and throughput). 
The \cwnd{} lower bound logic in current DCTCP implementation can have better scalability when MTU size is smaller\todo{need this?}.

\tightparagraph{More buffer pressure}
Second, we want to check whether \acdc{} can interact well with the switch's dynamic 
buffer allocation scheme. To this end, we create a scenario where almost every port is congested. 
We split 48 containers into 2 groups: group A and group B. 
Group A has 46 containers and Group B has 2 (denote as B1 and B2). 
Each of the 46 containers in Group A sends and receives 4 concurrent flows 
(i.e., container $i$ sends to range [$i+1$, $i+4$] mod 46). 
Meanwhile, all of the containers in Group A sends to container B1 so we can create a 46 to 1 incast. 
Finally, 47 out of 48 ports on the switch are congested. 
We measure the TCP RTT between B2 and B1 (i.e., TCP RTT of the traffic traversing the most congested port) and 
the results are shown in Figure~\ref{sockperf_pressure_incast}. 
The average throughputs for Default, DCTCP, and Ours are 214, 214 and 201 Mbps respectively, 
all with a fairness index greater than 0.98. 
Default has an average packet drop rate of 0.34\% but the most congested port has a drop rate as high as 4\%. 
This is why the 99.9$^{th}$ percentile TCP RTT for Default is very high. 
The packet drop rate for both DCTCP and \acdc{} is 0\%.
