\section{Background and Motivation}
\label{background}
\keqiang{we may put too much weight on performance isolation or bandwidth allocation,
i understand the ``modularity'' argument, but still, too much about bandwidth allocation work.
something like Mogul's "research (e.g., Proteus and Cicada) has shown how to do
this for network-bandwidth demands, but cloud tenants
may also need to meet latency objectives, which in turn
may depend on reliable limits on network latency, and its
variance,  within the cloud providers infrastructure" seems to be okay here.
After reading this section, people need to know what problem we are going to solve and 
why they are important.}

\subsection{Network Latencies in Datacenters}
Network latency is a critical performance metric for 
datacenters because it directly determine whether customers' System Level Objectives (SLO) can be met or not, 
especially for time-critical applications such as remote procedure calls (RPCs). 
Violations of SLOs affect customer experience and can cause revenue loss for cloud providers~\cite{dean2013tail}.~\cite{wang2010impact} measured RTTs for small and medium instance pairs in Amazon EC2 and show that 
RTTs have very high variance (ranging from around 0.1 millisecond to more than 10s of milliseconds). 
More recently,~\cite{mogul2015inferring} measured TCP latencies in 3 cloud providers and 
reported that TCP latency has very high variance and 99.9$^{th}$ percentile latency 
is an order of magnitude higher than the median.

The latency of traversing a single switch, NIC and OS network stack is 10--30$\mu$s, 
2.5--32$\mu$s and 15$\mu$s respectively~\cite{rumble2011s}.
So, in-network queueing latency which is caused by network congestion 
is considered as the major contributor to high end-to-end 
network latency and its variance~\cite{jang2015silo,mittal2015timely}.
For example, Google reported that in their extremely high capacity datacenter fabric, 
high congestion drops were observed when the network utilization approached 25\%~\cite{singh2015jupiter}.
To achieve low latency datacenter network, 
a set of research proposals have been proposed, such as DCTCP~\cite{alizadeh2011data}, 
HULL~\cite{alizadeh2012less}, TIMELY~\cite{mittal2015timely}, 
DX~\cite{lee2015accurate}, Silo~\cite{jang2015silo}, DCQCN~\cite{zhu2015congestion} etc.

%Latencies can be high due to congestion.
%Studies show large variance in latencies.
%Relate to our scheme. Google SIGCOMM 2015 paper.

\subsection{Congestion Control for Datacenter Network}
CUBIC, etc are inefficient. Variety of schemes to 
solve unique constraints within the datacenter. List all
TCP variants. Espeically talk about DCTCP. Relate to
ours scheme.
Bad TCP behaviour? Unfairness? 

\subsection{Performance Isolation}
\keqiang{here, we can say that today's cloud providers only 
have limited performance isolation properties, a commonly used
yet practical solution is to setting a ``celling'' bandwidth for 
different types of VM instances. 
Our scheme is immediately deployable for today's datacenter networks.
Even for the research proposals that 
aim to provide more strict bandwidth guarantees, they mainly or solely focus
on bandwidth guarantees. Our scheme is modular and compatible with those proposals.
Finally, as measurement works~\cite{roy2015inside,alizadeh2011data,greenberg2009vl2} 
on real datacenter network traffic show, 
most of the flows in datacenter are very small and busty. 
So providing bandwidth guarantee will benefit those bandwidth-hungry applications. 
For time-critical services which usually are small in nature, 
reducing in-network queueing latency is equivalent or even more important.
}

Datacenter providers must find ways to isolate the performance of different
tenants and applications so that one tenant or application cannot take
an unfair share of common resources, like CPU, memory and the network. And while
most providers (Amazon EC2, Microsoft Azure, Google Cloud Platform) provide logical
clusters for high-bandwidth, low-latency networking and dedicated instances, there still does not 
exist strict bandwidth guarentees over the network fabric. The absence of
guarentees means that tenant cross-traffic can induce delays due to
the switch's queue building up. This motivates the need for a simple, modular
scheme to reduce congestion in the network. The scheme should work in the absence
of any performance isolation schemes, and also with limited and even fine-grained
performance isolation. Our scheme is designed to easily drop in to current infrastructures
and performance isolation schemes. Say our scheme is light-weight and sits in virtual 
switch. 

Even though fine-grained network performance isolation is not widely adopted, there
has been significant studies into how to allocate network bandwidth in datacenter networks.
These schemes aim to either guarentee bandwidth for tenants or find ways to proportionally
allocate network resources. Their goals are achieved by either using sophiscated queueing 
techniques in the network switch or rate-limiting on end-hosts. Switch implementations
are typically difficult to scale. End-host rate-limiters typically focus on throughput, but
do not focus on latency. For example, CUBIC in incast has the same throughput as DCTCP, but the
latencies are much different because CUBIC fills the switch queues and DCTCP can proactively
avoid switch build-up. In order to provide low latency, such schemes usually provide 
bandwidth headroom of 5-10\%, reduce rates when line-rate is nearing or sacriface small amount
of bandwidth (probalby fits with first). Doing so is unnecessary if we have an appropiate
transport congestion control algorith, as DCTCP has been shown to achieve line-rate while
keeping queues low. Hence, our scheme of implementing congestion control in OVS is largely
complimentary to prior bandwidth allocation schemes. The schemes can provide upper-bounds
on the sending rates of flows or tenants in the network and the actual transport congestion
control can be hanled by our scheme. Need to talk about how using
rate-limiters have has high overhead and using windowing based scheme is much simpler? Provide
a footnote about Seawall here to forward reference discussion to related works? Probably need
to hit on fact that we don't change VMs, have tight control loop and low-cost, scalable way
to implement rate-limiting.
 

Need to talk about how our scheme works in absence of bandwidth allocation, but
also in cases where tenants/flows have teh same service levels/QoS guarentees.



\subsection{Rate Limiters vs TCP Windowing}
Rate limiting is a commonly used technique to prevent end-points from 
sending out too much traffic into the network. 
Rate-limiting can be achieved in hardware (NIC or edge switches) or 
software (\eg{}, Linux HTB) or hybrid mode. 
The known limitation is that commodity NICs have limited number of rate limiters 
which are insufficient especially as the server consolidation trend is growing. 
Lack of enough rate limiters causes head-of-line blocking issues for the traffic 
that are classified into the same class. 
SENIC~\cite{radhakrishnan2014senic} benchmarked the overhead of pure software-based Linux HTB and 
found that it incurs high kernel CPU overhead and is unable to handle more than 
6.5Gbps aggregated traffic (with MTU size 1500 bytes). So SENIC proposed to re-engineer 
the role of software and hardware and let host CPU only classify and 
enqueue packets into per-class queues in host memory and let NIC hardware perform 
precise packet scheduling such that SENIC can achieve both high scalability and low CPU overhead. 
But SENIC needs to replace NIC hardware on each server.
Silo~\cite{jang2015silo} proposed an improved pure software-based pacer but Silo's pacer requires to disable TSO on 
the hypervisor and still needs to use about one CPU core 
(which otherwise can be rented to customers) for 
rate limiting purpose to handle 10Gbps line rate. 

In this paper, we will explore another possibility---using TCP windowing to 
control network latencies (especially tail latencies).
Initially, TCP only has \rwnd{} to achieve flow control 
(i.e., prevent sender from overflowing receiver's buffer).
In 1988, Jacobson~\cite{jacobson1988congestion} identifed the causes of ``congestion collapse''
and added \cwnd{} and congestion avoidance and control algorithms to fix the issue for the Internet.
In the datacenter network environment 
(hypervisors, containers\todo{is this correct?}, 
and switches owned by the cloud provider; VMs rented to customers), 
\rwnd{} can be repurposed to achieve two goals---
flow control and ``transport enforcement'', in one single parameter.

\subsection{Virtualization \& Virtual Switch}
Open vSwitch (OVS)~\cite{ovs-website}, Cisco Application Virtual Switch~\cite{cisco-avs}, 
and VALE~\cite{rizzo2012vale}.
zOVN~\cite{crisan2013got} identified the packet loss problem in virtual networks and
proposed zVALE virtual switch to achieve lossless overlay virtual networks.
Open vSwitch (OVS) acceleration in the NIC~\cite{cavium-nic,netronome-nic}.

Also, link with Microsoft's ``end-host SDN'' concept---see SIGCOMM'15 and ONS keynote..
