\section{Background and Motivation}
\label{background}


\subsection{Network Latencies in Datacenters}
Network latency is a critical performance metric for
datacenters because it directly determines whether application or customer service level agreements (SLA)
can be met. Today's datacenters host many online data intensive (OLDI) applications like search,
advertising, analytics and~\todo{XXX} that require high bandwidth and low latency.
Large tail latencies often violate the tight timing constraints required by OLDI SLAs at scale, and
have been shown to impact customer experience, result in
revenue loss~\cite{alizadeh2011data,dean2013tail}, and degrade application performance~\cite{jang2015silo,qjump}.
Tail latencies are often caused by network congestion.
The latency of traversing a single switch, NIC and OS network stack is 10--30$\mu$s,
2.5--32$\mu$s and 15$\mu$s respectively, but a congested port
on a network switch can consume significant shared memory, causing orders-of-magnitude
higer latency~\cite{rumble2011s}.
Congestion is caused by many reasons such as imperfect load balancing,
switch/link upgrades or failures and network oversubscription and its occurance in
datacenters today is not rare. For example, Google reported that in their datacenter fabric
congestion-based drops were observed when the network utilization approached 25\%~\cite{singh2015jupiter}.
Other studies have shown high variance and substantial increase in the 99.9$^{th}$ percentile latency
for round-trip times in today's datacenters~\cite{wang2010impact,mogul2015inferring}. Therefore,
significant motivation exists to ensure datacenter fabrics reduce the impact of congestion in order to
limit tail latencies.

%Studies have shown that while CUBIC can achieve
%high bandwidth, it does so at the cost of aggressively filling up the switch buffers in the network.
The performance of TCP, specifically its congestion control algorithm, is widely
known to significantly impact network performance.
As a result, the performance of TCP in datacenters has been widely
studied and many new protocols have been proposed~\cite{alizadeh2011data, stephens2014practical, wu2010ictcp,
mittal2015timely, jose2015high}. Specifically, DCTCP~\cite{alizadeh2011data} achieves high throughput and low
latency by adjusting a TCP sender's rate based on the fraction of packets that experience congestion. In DCTCP,
switches are configured to mark packets with an ECN bit when their queue lengths exceed a threshold. By proportionally
adjusting the rate of the sender based on the fraction of ECN bits received, DCTCP can keep queue lengths low while
allowing flows to maintain high throughput. It has also been shown to increase fairness and stability over other schemes.
~\eric{cite}

\subsection{Limitations of Bandwidth Allocation}
Datacenter providers must find ways to isolate the performance of different
tenants and applications so that one tenant or application cannot take
an unfair share of common resources, like CPU, memory or the network.
In order to provide a degree of performance isolation in the network, datacenter operators can implement
a variety of bandwidth allocation schemes. Bandwidth allocation schemes range
in their complexity. Simple static rate limiters found
on many default public cloud images enforce an upper-bound the bandwidth available
to different classes of VMs. More complex schemes investigate how to allocate network bandwidth in datacenter
networks by either guarenteeing or proportionally
allocating bandwidth for tenants~\cite{rodrigues2011gatekeeper,Ballani2011oktopus,jeyakumar2013eyeq,shieh2011sharing,
Guo2010Secondnet,Popa2012Faircloud,Xie2012Proteus,Lam2012NetShare}. 
Regardless of their differences, these works mainly focus on {\em bandwidth} allocation with
little to no emphasis on latency~\footnote{Note Silo~\cite{jang2015silo}
guarentees bandwidth and delay by additionally enforcing VM admission and placement}.


Allocating bandwidth alone can be insufficient
to reduce latencies because different TCP stacks have a large impact on switch queueing delays.
As a simple example,
consider the topology in Figure~\ref{dumbbell_topology}. 
%If the sum of each flow's rate limter is greater than the capacity
%of the link, congestion will occur. CUBIC~eric{cite}, the default TCP congestion control algorithm in Linux, will fill
%the buffer of the congested output port, causing latencies to significantly increase. DCTCP, however, keeps buffers low.
Even in the case of a "perfect" allocation, where each flow is statically limited to 10 Gbps/5 flows = 2 Gbps,
the latency caused by queueing heavily depends on the deployed TCP stack. We show this in Figure~\eric{XXX}.
CUBIC~\cite{ha2008cubic}, the default TCP congestion control algorithm in Linux, will aggressively fill
the buffer of the congested output port, causing latencies to significantly increase. DCTCP, however, is able
to keep buffers low~\cite{alizadeh2011data}, allowing for a queueing latency that is an order of magnitude lower than CUBIC's. These differences exist despite the fact
that each scheme is able to achieve the same throughput over the congested link (Section~\ref{results}~\eric{Table 1}). 
Note that in Figure~\eric{XXX}, DCTCP is run in the absence of a static rate limiter, meaning that it is effective
in mitigating congestion's impact on latency.

Complex bandwidth allocation schemes that do not rely on simple admission policies must consider a host of
practical issues that make finding the "perfect" allocation difficult. 
Datacenter networks are incredibly dynamic in nature due to failures, multi-pathing, load-balancers, changing real-time
traffic patterns and even VM churn~\eric{last one correct?}. The fact that datacenter networks have incredibly high
bandwidths ($\geq$ 10 Gbps) makes the problem even harder to contend with. Therefore, as these schemes adapt to
prevailing changes in the network, it is necessary to ensure an inefficient bandwidth allocation does not 
impact the latencies of other tenants due to transient congestion. 
~\eric{this paragraph is incredibly hand-wavy. Not sure if it is strong enough. Some more results or citations to back
up our story would be nice. Also, need to double-back on the related works b/c some that do in-network switching may
not have some of these issues. Same with proportionally fair schemes. Also, will folks have the necessary
background in bandwidth allocation schemes to know what is going on? Do we have to give some details about them?}


\subsection{Benefits of Datacenter Congestion Control in vSwitch}
As outlined in the previous subsection, enforcing a datacenter-specific congestion control algorithm can help reduce queueing latencies in the network. 
In addition, implementing congestion control in the vSwitch has numerous other benefits. First, in a public datacenter environment where
VMs may deploy non-optimized TCP stacks, implementing congestion control in the vSwitch can provide an element of control without cooperation from
VMs. This is an important criteria in untrusted environments or simply in cases where tenants cannot update their TCP stack due to
various constraints (such as not being able to update their OS or a dependence on a certain library). Our work provides an important distinction from a pioneering work in tunnel-based congestion control, Seawall~\cite{shieh2011sharing}, in that we do not require any modifications to VMs. 

The next major benefit is that moving the congestion control algorithm within the virtual switch allows for a {\em unified}
congestion control algorithm to be implemented throughout the datacenter.
Ensuring all tenants have the same stack can help limit unfairness in the network. Unfairness arises
when stacks are handled differently in the fabric or when conservative and aggressive
stacks coexist. In~\cite{judd2015nsdi}, it is shown than ECN and non-ECN flows do not exist gracefully on the
same fabric because packets belonging to non-ECN flows encounter severe packet drops when their packets
exceed queue thresholds. Ideally, tenants shouldn't suffer based on a such a simple configuration issue.
Additionally, stacks with different congestion control algorithms may not gracefully coexist on the same fabric.
For example, Figure~\ref{tput_fairness_coexistence} shows the performance of five different TCP flows over the topology in
Figure~\ref{dumbbell_topology}. Each flow has a different congestion control algorithm, all of which are available in today's Linux
distribution. In this case the more aggressive stacks like TCP Illinois and TCP HighSpeed
always achieve higher bandwidth. A tenant with the same standing as another tenant in the datacenter should not be able
to achieve higher bandwidth by simply altering their stack.

Another benefit of moving congestion control to the vSwitch is that it allows for different congestion control algorithms to be used on
a per-flow basis. Currently, all flows within a VM are tied to the same congestion control algorithm. This severely limits
flexibility and forces tenants to optimize the performance a certain set of flows. For example, a webserver may choose a TCP stack that optimizes
WAN performance, instead of the back-end performance within the datacenter. As studies have shown that TCP can be optimized for datacenters~\cite{alizadeh2011data, stephens2014practical, wu2010ictcp,
mittal2015timely, jose2015high}, WAN environments, and even
~\eric{one more example, wireless/60Ghz/free-space optics?}, selecting a per-flow TCP stack has the potential to 
enhance network performance. By moving congestion control to the vSwitch, administrators can assign a specific congestion control
algorithm to each flow, optimizing the network performance of its clients in a seamless manner. 

Finally, congestion control functionality is easy to port. Note that while the entire TCP stack may seem complicated and prone to high-overhead,
the congestion control aspect of TCP is relatively light-weight and easy to implement. Indeed, studies have
shown that most overhead comes from TCP's buffer management~\cite{optimize-tcp-receive}. The actual congestion control implementations in Linux
are modular: DCTCP's Linux congestion control resides in {\tt tcp\_dctcp.c} and is only about 350 lines of code. Given
the simplicity of congestion control, it is relatively easy to move its functionality to another
layer. In~\acdc{}, we don't modify VMs so congestion control is run both in the vSwitch and in the VM. However, because congestion
control is light-weight, the penalty imposed for running it twice is low: our benchmarks in Section~\todo{XXX} show the computational overhead of our scheme is less than 2\%.


\subsection{Co-locating Rate-limiting and Congestion Control~\eric{change title?!}}

Bandwidth allocation schemes typically rely on rate-limiting the end host or deploying
scheduling mechanisms in the switch to provide performance isolation. Rate-limiting schemes have various trade-offs:
software-based schemes can suffer from high computational overhead~\cite{radhakrishnan2014senic} and hardware-based
schemes must scale.~\acdc{} aims to limit the burden on the rate-limiter by working within a simple architechture. One
rate-limiter can be configured per VM in order to ensure performance isolation.~\acdc{} enforces how many packets a
flow can send to ensure queueing latency isn't increased when a flow becomes network bound. The schemes work in
concert, in that traffic flowing from a VM is bound by the minimum enforcement of the rate-limiter and our scheme.

A key design constraint of~\acdc{} is to make it modular in nature. This means that our scheme should work with a variety
of bandwidth allocation schemes and their associated rate-limiters (and also in the absence of both). In order to 
achieve this goal,~\acdc{} satisfies a variety of constraints. First, it is computationally light-weight in order to minimize 
the overhead of its adoption. 
Second, it doesn't require any changes to VMs or network hardware so it can be deployed in current and future networks. 
Third, our scheme works in the absence of specific topology information and works over arbitrary topologies. 
Fourth, admins need not possess information about tenant or flow demands. Our 
implementation within the vSwitch is modularized such that it can easily be co-deployed with other schemes that may also
alter the vSwitch or components within the hypervisor's networking stack.
In short, we design our scheme to be complementary in nature to the large body of bandwidth allocation and rate-limiting schemes.
~\eric{this needs work.}




\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_15k.pdf}
                \caption{MTU = 1.5KB.}
                \label{effectiveness_15k}
        \end{subfigure}
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_9k.pdf}
                \caption{MTU = 9KB.}
                \label{effectiveness_9k}
        \end{subfigure}
        \caption{Using \rwnd{} can effectively control throughput.
                Experiments are conducted on a 10G testbed. TCP CUBIC but New Reno shows similar results.
                Linux 3.18.0. We control maximal \rwnd{} value by modifying the receiver's advertised window size in TCP ACKs
                in the Open vSwitch. We control maximal \cwnd{} by specifying ``snd\_cwnd\_clamp" value in Linux TCP.}
        \label{rwnd_effectiveness}
\end{figure}
