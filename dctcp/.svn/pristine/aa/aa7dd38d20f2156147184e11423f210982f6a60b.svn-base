\section{Implementation}
\label{impl}
This section outlines relevant~\acdc{} implementation details. We
implemented~\acdc{} in Open vSwitch (OVS) v2.3.2~\cite{ovs-website} and
added about 1200 lines of code (many are debug/comments). 
\crs{When a packet is processed by OVS, \acdc{} first classifies 
it based on 5-tuple into flow. Then, per-flow congestion 
control-related states (mentioned in \cref{design}) are updated. 
Flow states are maintained in hash tables. 
The implementation uses several standard techniques
to reduce overhead.
First, 
we leverage the fact that there are many more table lookup operations 
than table insertion or deletion operations. Thus, Read-Copy-Update (RCU) 
hash tables~\cite{guniguntala2008read} and 
per-flow {{\tt spinlock}} are used to reduce the overhead due to locking.
Second,~\acdc{} works with NIC offloading features (\ie{}, TSO and GRO/LRO) 
that are commonly supported by commodify NICs. Therefore, 
\acdc{} manipulates on TCP segments, instead of MTU-sized packets,
which helps keep CPU overhead low.
Third, we leverage NIC checksumming so the TCP checksum does not have to be
recomputed after header fields are altered 
(note computing TCP checksum is a computationally heavy task in software). 
Based on the running states for each flow, we can enforce DCTCP congestion 
algorithm by modifing \rwnd{} field in each incoming ACK using a {{\tt memcpy}} 
operation.
Finally, we also quantify that each TCP connection requires only 320 bytes
of memory. Memory is released when a TCP connection is finished. 
}

While our scheme is implemented in software, we are currently investigating
the possibility of implementing~\acdc{} in the NIC. Today, "smart-NICs"
implement OVS-offload functionality~\cite{cavium-nic,netronome-nic}, and 
naturally provide a mechanism to support hypervisor bypass (\eg{}, SR-IOV). 
%We have also considered designing an~\acdc{}-enabled middlebox to support
%legacy systems that do not run OVS and cannot upgrade their NICs.


%
%Standard OVS kernel datapath LoC: 2360
%\acdc{} kernel datapath LoC: 3590
%
%\tightparagraph{UDP traffic} How to handle.
%Mention VxLAN traffic too.
%\keqiang{and IPsec}
%
%\tightparagraph{No vSwitch}
%\keqiang{title should be hypervisor-bypass?}
%Use middleboxes (for DB server).
%Use NIC (for SR-IOV).
%Hypervisor bypass (e.g., SR-IOV), where TCP traffic is sent to the NIC directly without
%going through hypervisor. First, as noted by~\cite{shieh2011sharing}, ``loss of the security and
%manageability features provided by the software virtual switch has limited
%the deployment of direct I/O NICs in public clouds''. Second, based on techniques like Intel
%DPDK~\cite{intel-dpdk} and ``smart NICs''~\cite{cavium-nic,netronome-nic}, we believe that low latency
%congestion control enforcement schemes like \acdc{} can also be
%employed for hypervisor bypass use cases.
%We need to worry about legacy systems and non-VM systems. For instance, a database or storage device that may not have OVS installed on it.
%We need to talk about either a middlebox or that this percentage of traffic is low? Or implement in NIC (especially one with OVS offload?).
%
%
%\tightparagraph{Little CPU and memory overhead}
%In our implementation, first we leverage the Read-Copy-Update (RCU)~\cite{guniguntala2008read} enabled hash tables
%to keep per-flow states (such as ``snd\_una'' and ``snd\_nxt''). RCU technique is also employed by
%Open vSwitch's kernel datapath and it helps improve processing speed for ``read-heavy''
%workloads (\ie{}, inserting new flows is much less frequent than looking-up existing flows) on
%shared-memory multiprocessor systems.
%Second, \acdc{} processes on ``segment'' level instead of ``packet'' level due to
%NIC offloading features (TSO at the sender side and GRO/LRO at the receiver side).
%Third, we also leverage the NIC checksumming offloading feature such that
%we do not need to compute checksums after we change TCP/IP header fields.
%Our microbenchmarks (\cref{micro}) show that \acdc{} incurs very little additional CPU overhead (less than 4\%) to
%support 10Gbps line-rate, even it is fully implemented in software.
%We are currently implementing \acdc{} on Cavium's programmable NICs~\cite{cavium-nic},
%where we can entirely offload the computational overhead to hardware. Therefore, we believe
%\acdc{} can support even higher line rates (\eg{}, 40Gbps).
%In our implementation, each TCP connection takes 320 bytes in the hash tables,
%so it takes around 3.2MB even there are 10K concurrent connections.
%{~\keqiang{i think we may want to mention that we use both FACK and PACK because we want to be compatible with TSO and try to minimize the CPU/traffic overhead.}
