\section{Related Work}
\label{related}

We summarize the related work on improving data center/cloud network's \emph{performance} below. 
We focus on the following categories of works: 
1)Datacenter network architectures to provide massive bandwidth and strong fault-tolerance,
2)traffic engineering and load balancing within the data center to 
meet throughput and latency demands of the cloud-hosted workloads, 
3)congestion control and transport protocols to mitigate the extent of network congestion 
and reduce network latency and 4)improving end-users' Internet access to cloud-hosted services.

{\bf Datacenter Network Architectures}
Many datacenter network architectures have been proposed. 
The seminal work FatTree~\cite{fattree} (FatTree is a 3-stage Clos network~\cite{clos1953study}) 
proposes to use cheap Ethernet switches to construct FatTree topologies to 
reduce cost and supports tens of thousands of servers with full bisection bandwidth. 
For example, using 48-port GigE Ethernet switches, FatTree topology can support 
up to 27,648 servers and there are 576 equal-cost paths between any 
given hosts pair that are not located in the same pod. 
ECMP can be employed to spread flows across multiple links to achieve load balancing. 
Because of massive multipathing, FatTree topology's bandwidth degrades gracefully 
when link failure happens. Similarly, VL2~\cite{vl2} is also a 3-stage Clos topology, 
the difference from FatTree is that VL2 only uses 1Gbps links between ToR (Top-of-Rack) switches 
and servers and uses 10Gbps elsewhere. This reduces wiring complexity of the topology. 
VL2 uses centralized ARP and link-state routing protocol. VL2 proposes to decouple 
locator IP address and application IP address to provide virtual layer 2 networks 
using IP-in-IP encapsulation which inspired the development of VXLAN~\cite{mahalingam2014vxlan}. 
Portland~\cite{niranjan2009portland} tries to provide layer 2 semantics (i.e., an VM can has any IP address, 
regardless of its location), so it uses pseudo MAC address to forward packets and
 when packets arrive at the destination edge switch pseudo MAC is rewritten to actual MAC. 
Pseudo MACs are assigned based on the location of the edge switch 
(the goal of positional pseudo MAC is to use wildcard rules to reduce the number of 
rules needed in the switch). A fabric manager is used to perform ARP and failure handling. 
When failure occurs, location discovery message exchanged between switches will 
discover link failures and report to the fabric manager, 
then the fabric manager updates forwarding rules in the affected switches to 
bypass the failed path. The topology is FatTree (3-stage Clos network) and 
all the links have the same speed (1Gbps). 
Multipathing is assumed to be achieved by ECMP. 
Jupiter Rising~\cite{singh2015jupiter} presents the evolution of Google's datacenter networks in the last decade. 
The key ideas are: using multi-stage Clos topology to scale out, 
using merchant chips to build switching blocks to reduce cost and 
using centralized routing protocol to manage the routing behavior. 
Routing protocol (i.e., firepath) is centralized and upon failures, 
neighbor discovery messages exchanged between switches will identify link down event and 
firepath will push the topology state changes to switches. 
Then firepath agents on each switch individually re-calculate the forwarding states to 
bypass failed links. Different from Portland, in firepath, 
topology state is managed centralized but routing calculation is distributed. 
The routing convergence time after link failure is hundreds of ms to several seconds. Multipathing is achieved by ECMP.

{\bf Traffic Engineering and Load Balancing in Datacenters}
MPTCP~\cite{dc-mptcp} is a transport protocol that uses subflows to
transmit over multiple paths.
Transport layer solutions such as MPTCP require widespread adoption and are difficult to
enforce in multi-tenant datacenters where customers often deploy
customized virtual machines (VMs).
CONGA~\cite{alizadeh2014conga} and Juniper VCF~\cite{juniper-vcf} 
employ congestion-aware flowlet switching~\cite{flowlet} on
specialized switch chipsets to effectively load balance the network.
CONGA~\cite{alizadeh2014conga} and Juniper VCF~\cite{juniper-vcf}
need to replace the edge switches in the data center.
RPS~\cite{packetspray} and DRB~\cite{cao2013drb} perform 
per-packet load balancing on symmetric 1 Gbps networks
at the switch and end-host, respectively.
Per-packet load balancing can incur significant end-host overhead for DRB
if not resorting to jumbo frames.
Packet reordering problem is not well considered in both RPS and DRB.
RPS and DRB may perform worse than ECMP in case of topology asymmetry.
Hedera~\cite{al2010hedera}, 
MicroTE~\cite{benson2011microte} and Planck~\cite{rasley2014planck} 
use centralized traffic engineering to
reroute traffic based on network conditions.
Fastpass~\cite{perry2014fastpass} employs a centralized arbiter to
schedule path selection for each packet.
Centralized schemes are fundamentally reactive to congestion, and are
very coarse-grained due to the large time constraints of their control
loops~\cite{al2010hedera}, require extra network
infrastructure~\cite{rasley2014planck} or 
suffer from a single point of failure~\cite{perry2014fastpass}.
FlowBender~\cite{kabbani2014flowbender} reroutes flows when congestion is detected by end-hosts 
but network congestion already happened before FlowBender can react to it.

{\bf Congestion Control and Transport Protocols in Datacenters}
DeTail~\cite{zats2012detail} is a cross-layer network stack designed to 
reduce the tail of flow completion times.
DCTCP~\cite{alizadeh2011dctcp} is a transport protocol that uses the portion of marked packets
by ECN to adaptively adjust TCP's congestion window to reduce switch buffer occupancy.
Thus, DCTCP can reduce flow completion time.
HULL~\cite{alizadeh2012hull} uses Phantom Queues and congestion notifications to 
cap link utilization and prevent congestion.
~\cite{vasudevan2009safe} advocates to reduce TCP $RTO_{min}$ 
(200 ms in default in Linux) value to fine-grained values 
(sub-millisecond level) to solve the TCP incast problem~\cite{chen2009incast}. 
ICTCP~\cite{wu2013ictcp} proposes to monitor TCP throughputs at the receiver side and 
adaptively adjust the TCP receiver window size to reduce TCP throughput 
and avoid packet drops caused by TCP incast congestion.
TCP-Bolt~\cite{tcp-bolt} identifies three serious performance issues in 
lossless Ethernet: large buffering delays, TCP throughput unfairness and 
head-of-line blocking. The authors of TCP-Bolt 
propose a variant of DCTCP (DCTCP without slow start to 
best utilize the benefits of lossless Ethernet) to solve the three key performance issues.
DCQCN~\cite{zhu2015rdma} also attacks the large buffering delay, 
TCP unfairness and head-of-line blocking problems in lossless Ethernet, 
but DCQCN works for a slightly different scenario: 
Remote Direct Memory Access (RDMA) over drop-free 
IP-routed networks and 40Gbps line-rate for 
highly demanding application. 
Similar to TCP-Bolt, DCQCN applies DCTCP-like congestion control 
and switch marking to lossless network to solve the three 
above mentioned performance issues.
DCQCN is implemented on Mellanox NICs to 
reduce CPU overhead to meet the high speed (40G) requirement. 
TIMELY~\cite{mittal2015timely} and DX~\cite{lee2015accurate} 
revisit the possibility of using RTT to 
perform congestion control for data center networks. The conventional wisdom 
is that RTT-based congestion control is not suitable for 
data center networks because of its low latency characteristic. 
However, with modern network interface cards (NICs), it is possible to use 
hardware timestamps provided by the NIC to get 
accurate RTT measurement to perform congestion control for the data centers. 

\iffalse 
{\bf Applying SDN Techniques to Improve the Performance of Data Center Networks}
B4~\cite{jain2013b4}, SWAN~\cite{hong2013achieving} and 
BwE~\cite{kumar2015bwe} are a series of proposals 
that apply the software-defined networking (SDN) technique to improve 
link utilization for inter-DC networks or data center WANs. 
SDN is a good fit for data center WAN optimization because 
a logically centralized controller has the global view of network 
resources and traffic demands. Therefore, SDN technique can 
naturally maximize the average link utilization.  
A key research point that is less explored in 
SDN domain is ``how fast we can reprogram the switch’s data plane 
and whether the existing switches can react fast enough to 
meet the requirements of proposed control programs''. 
Devoflow~\cite{curtis2011devoflow} shows that the rate of statistics
gathering is limited by the size of the flow table and that statistics
gathering negatively impacts flow setup rate. More recently, two
studies~\cite{rotsos2012oflops,huang2013high} provide a more in-depth look into
switch performance across various vendors.  In~\cite{rotsos2012oflops}, the
authors evaluate 3 commercial switches and observed that switching
performance is vendor specific and depends on applied operations,
forwarding table management, and firmware. In ~\cite{huang2013high}, the
authors also study 3 commercial switches (HP Procurve, Fulcrum,
Quanta) and found that delay distributions were distinct, mainly due
to variable control delays.
Some studies have considered approaches to mitigate the overhead of SDN
rule matching and processing. DevoFlow~\cite{curtis2011devoflow} presents
a rule cloning solution which reduces the number of controller
requests being made by the switch by having the controller set up
rules on aggregate or elephant flows. 
DIFANE~\cite{yu2011scalable} reduces flow set up latency by splitting
pre-installed wild card rules among multiple switches and therefore
all decisions are still made in the data plane. 
vCrib~\cite{moshref2013vcrib} automatically partitions and places rules at both hypervisors and
switches, but their goal is to reduce computational load on the
host hypervisor, while we wish to reduce path setup latency by enabling fast
parallel execution of updates.
Lastly, Dionysus~\cite{jin2014dynamic} optimally schedules a set of rule updates
while maintaining desirable consistency properties (e.g., no loops and no
blackholes).
\fi

{\bf Improving End-users' Internet Access to Cloud-hosted Services}
Reducing the latency for Internet end-users' access to the 
cloud-hosted services is critical to revenue.~\cite{flach2013reducing} observes that TCP's timeout-driven 
recovery causes web transfers take 5 time longer than average. 
Based on this observation, it proposes three kinds of loss recovery 
schemes to reduce the web access latency. Their experiments show that 
they can cut the average latency by 23\% and 99th percentile latency by 47\%. 
Ananta~\cite{patel2013ananta} proposes a software-based load balancer to 
load balance the north-south traffic (\ie{}, the traffic between 
Internet users and the services in the cloud) such that the users 
requests can be distributed properly among a large set of servers 
and user access latency is reduced.
Amazon Route 53~\cite{amazon-route53} is a DNS system and it 
can direct the end-users’ requests to the services 
running in Amazon AWS. Route 53 has the ability to direct the requests to 
the servers with the least latency to the end-users so it 
can help improve user experience. 
