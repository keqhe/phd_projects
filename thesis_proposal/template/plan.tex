\section{Proposed Work}
\label{plan}

\subsection{Congestion control enforcement for low latency datacenter networks}

\iffalse
There are many practical performance hurdles when we build high throughput and low latency data center 
networks to accommodate high-demanding applications 
(\eg{}, in-memory computing needs fast network to finish computing tasks). 
DCTCP~\cite{alizadeh2011dctcp} is a transport protocol designed to reduce the network buffering 
(thus reducing network latency) for data center networks. But, as we learned, 
in a real production data center network, it is hard to assume that all the TCP stacks are 
under the cloud provider's control. This is the case for infrastructure-as-a-service (IaaS) cloud, \eg{}, 
the default TCP protocol used by Amazon EC2 instances is TCP CUBIC at the time of writing. 
From the cloud provider's view, first, 
we want to achieve the goal of low latency for all of the tenant's traffic. Therefore, 
there is a need to enforce DCTCP-like transport protocol on every host in the network. 
Second, applying DCTCPâ€™s ECN setting to data center switches seriously harms traditional TCP traffic.
As pointed out by~\cite{judd2015dctcp}, when TCP traffic is mixed with DCTCP traffic and ECN marking
is turned on, there are two practical performance issues: 1)DCTCP can gain almost all
the bandwidth while TCP can only get very little share and 2)TCP or DCTCP's connection
establishment probability is decreased sharply with the increase of the number of
competing flows in the network. All these two issues are related to the switch's
AQM (Active Queue Management) scheme---it simply drops non-ECT (ECN-Capable Transport)
packets when the switch's queue length is (even slightly) larger than the threshold.
One of the solutions is to apply ECT to all packets at the virtualization edge
(no matter it is TCP CUBIC or DCTCP, no matter it is control packets or data packets).
We undo the ECT marking properly when the packet arrives the destination host.
We propose to build a universal congestion control enforcement component in the virtualization layer or the NIC to 
boost the network performance (both low latency and high throughputs) for all kinds of traffic. 
The major challenge in achieving this goal is how we can solve this problem in a scalable and light-weight manner. 
We plan to implement this congestion control enforcement component on a real testbed and 
run various kinds of tests to validate its performance.
Finally, we also hope that our congestion control enforcement
logic is beneficial to the lossness \emph{ Data Center Bridging } or \emph{ Converged Ethernet }
environment where there are serious bufferbloat (hence increased network latency) and
fairness issues~\cite{tcp-bolt,zhu2015rdma}.
It can also improve the network performance for over-conservative transport protocols such as TCP+ECN.
\fi

Multi-tenant datacenters are a crucial component of today's computing ecosystem. Large providers, such as Amazon, Microsoft, IBM, Google and Rackspace, support
a diverse set of customers, applications and systems through their public cloud offerings. These offerings are successful in
part because they provide efficient performance to a wide-class of applications running on a diverse set of platforms. Virtual
Machines (VMs) play a key role in supporting this diversity by allowing customers to run applications in a wide variety of
operating systems and configurations.

And while the flexibility of VMs allows customers to easily move a vast array of applications into the cloud, that same flexibility inhibits the
amount of control a cloud provider can yield over VM behavior. For example, a cloud provider may be able to provide virtual networks or enforce rate limiting
on a tenant VM, but it cannot control the TCP/IP stack running on the VM. As the TCP/IP stack considerably impacts overall network performance, it
is unfortunate that cloud providers cannot exert a fine-grained level of control over one of the most important components in the networking stack.

Without having control over the VM TCP/IP stack, datacenter networks remain at the mercy of inefficient, out-dated or misconfigured TCP/IP stacks.
TCP behavior, specifically congestion control, has been widely studied and many issues have come to light when its behavior is not optimized. For example,
network congestion caused by non-optimzed stacks can lead to loss, increased latency and reduced throughput.

Thankfully, recent advances in optimizing TCP stacks for datacenter environments have shown that both high throughput and low latency can be
achieved through novel TCP congestion control algorithms. Works such as DCTCP~\cite{alizadeh2011dctcp} and TIMELY~\cite{mittal2015timely} show great promise in providing high
bandwidth and low latency by ensuring that network queues in switches do not fill up. And while these stacks are deployed in many of today's
private datacenters~\cite{singh2015jupiter,judd2015dctcp}, ensuring that a vast majority of VMs within a public datacenter will update their TCP stacks
to this new technology is a daunting, if not impossible task.

We plan to explore how operators can regain control of TCP's congestion control, regardless of the TCP stack
running in a VM. Our aim is to allow a cloud provider to utilize advanced TCP stacks, such as DCTCP, without having
control over the VM or requiring changes in network hardware. We propose implementing congestion control in the virtual switch
(vSwitch) running on each server. Implementing congestion control within a vSwitch has several advantages.
First, vSwitches naturally fit into datacenter network virtualization architectures and are widely
deployed~\cite{pfaff2015design}. Second, vSwitches can easily monitor and modify traffic passing through them.
Today vSwitch technology is mature and robust, allowing for a fast, scalable,
and highly-available framework for regaining control over the network.

Implementing congestion control within the vSwitch has numerous challenges. First, in order to ensure adoption rates are high, the
approach must work without making changes to VMs.
Hypervisor-based approaches that do not modify VMs typically rely on rate limiters to limit VM traffic. Rate limiters implemented in
commodity hardware do not scale in the number of flows and software implementations incur high CPU overhead~\cite{radhakrishnan2014senic}.
Therefore, limiting a VM's TCP flows in a fine-grained, dynamic nature
at scale (10,000's of flows per server~\cite{moshref2013vcrib}) with limited computational overhead remains challenging.
Finally, VM TCP stacks may differ in the features they support (\eg{}, ECN) or the congestion
control algorithm they implement, so a vSwitch congestion control implementation should work under a variety
of conditions.

We propose a new technology that implements
TCP congestion control within a vSwitch to help ensure VM
TCP performance cannot impact the network in an adverse way. At a high-level, the vSwitch monitors all packets for a flow, modifies
packets to support features not implemented in the VM's TCP stack (\eg{}, ECN) and reconstructs
important TCP parameters for congestion control. vSwitch runs the congestion control logic specified by an administrator and then enforces an intended
congestion window by modifying the receiver advertised window (\rwnd{}) on incoming ACKs. A policing
mechanism (\ie{}, via dropping any excess packets not allowed by the calculated congestion window) ensures stacks cannot benefit from ignoring~\rwnd{} and can also be used for non-TCP traffic.

\subsection{Datacenter network architecture analysis and exploration}

Datacenter network architecture determines how scalable the network is, 
how resilient it is to link or switch failures and how easily the network 
can be incrementally deployed. Today's practice is that network architect needs to 
manually infer (usually based on experiences) many key characteristics related to 
the candidate network topologies. So we lack a scientific and complete method to 
evaluate different network topologies. Therefore, there is a need to build a network 
architecture analysis framework to help network architect analyze and compare 
candidate network topologies. To compare different network topologies, we need to 
set up metrics to quantify different network topologies. Our first goal is to 
identify a set of metrics (e.g., cost, wiring complexity, bandwidth, reliability, routing convergence) that 
can accurately quantify datacenter network topologies.  Also, we need to define a set of workloads 
and traffic patterns to run against the network. Given the metrics and workloads, 
we are very interested to answer questions like: how many hosts are disconnected when 
a TOR switch fails? How much bandwidth is lost when an aggregation switch fails? 
When a core switch fails? What about specific links? 

Analyzing the existing topologies such as VL2~\cite{vl2}, FatTree~\cite{fattree}, F10~\cite{liu2013f10}, 
Jupiter~\cite{singh2015jupiter} is our first step. Next, we want to investigate that using 
this analysis framework, whether we can gain insights to design better network topologies. 
A motivating example is F10, which identifies new striping patterns that can 
improve FatTree's fault-tolerance. Using our analysis framework, it will be much faster 
to explore new datacenter network topologies and exam the tradeoffs among different metrics 
for new topologies. Finally, we will investigate how we codesign routing protocol, 
load balancing schemes to best utilize the new topologies. 

\subsection{Timeline}

Table \ref{tab:plan} shows my plan for completion of the research.

\begin{table}[hc]
\begin{small}
\begin{center}
\begin{tabular}{lll}
Timeline & Work & Progress\\
\hline
          & Presto \& CloudMeasure & completed\\
Oct 2016 & congestion control enforcement (NSDI'16) & proposed\\
Jan 2017 & datacenter network architecture analysis and exploration (SIGCOMM'17)  & proposed\\
May 2017 & thesis defense & \\
\end{tabular}
\end{center}
\end{small}
\caption{Plan for completion of my research}
\label{tab:plan}
\end{table}

