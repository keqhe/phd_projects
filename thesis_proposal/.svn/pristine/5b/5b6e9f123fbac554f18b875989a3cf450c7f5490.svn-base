\section{Proposed Work}
\label{plan}

\subsection{Congestion control enforcement for low latency data center networks}

\iffalse
There are many practical performance hurdles when we build high throughput and low latency data center 
networks to accommodate high-demanding applications 
(\eg{}, in-memory computing needs fast network to finish computing tasks). 
DCTCP~\cite{alizadeh2011dctcp} is a transport protocol designed to reduce the network buffering 
(thus reducing network latency) for data center networks. But, as we learned, 
in a real production data center network, it is hard to assume that all the TCP stacks are 
under the cloud provider's control. This is the case for infrastructure-as-a-service (IaaS) cloud, \eg{}, 
the default TCP protocol used by Amazon EC2 instances is TCP CUBIC at the time of writing. 
From the cloud provider's view, first, 
we want to achieve the goal of low latency for all of the tenant's traffic. Therefore, 
there is a need to enforce DCTCP-like transport protocol on every host in the network. 
Second, applying DCTCPâ€™s ECN setting to data center switches seriously harms traditional TCP traffic.
As pointed out by~\cite{judd2015dctcp}, when TCP traffic is mixed with DCTCP traffic and ECN marking
is turned on, there are two practical performance issues: 1)DCTCP can gain almost all
the bandwidth while TCP can only get very little share and 2)TCP or DCTCP's connection
establishment probability is decreased sharply with the increase of the number of
competing flows in the network. All these two issues are related to the switch's
AQM (Active Queue Management) scheme---it simply drops non-ECT (ECN-Capable Transport)
packets when the switch's queue length is (even slightly) larger than the threshold.
One of the solutions is to apply ECT to all packets at the virtualization edge
(no matter it is TCP CUBIC or DCTCP, no matter it is control packets or data packets).
We undo the ECT marking properly when the packet arrives the destination host.
We propose to build a universal congestion control enforcement component in the virtualization layer or the NIC to 
boost the network performance (both low latency and high throughputs) for all kinds of traffic. 
The major challenge in achieving this goal is how we can solve this problem in a scalable and light-weight manner. 
We plan to implement this congestion control enforcement component on a real testbed and 
run various kinds of tests to validate its performance.
Finally, we also hope that our congestion control enforcement
logic is beneficial to the lossness \emph{ Data Center Bridging } or \emph{ Converged Ethernet }
environment where there are serious bufferbloat (hence increased network latency) and
fairness issues~\cite{tcp-bolt,zhu2015rdma}.
It can also improve the network performance for over-conservative transport protocols such as TCP+ECN.
\fi

Multi-tenant datacenters are a crucial component of today's computing ecosystem. Large providers, such as Amazon, Microsoft, IBM, Google and Rackspace, support
a diverse set of customers, applications and systems through their public cloud offerings. These offerings are successful in
part because they provide efficient performance to a wide-class of applications running on a diverse set of platforms. Virtual
Machines (VMs) play a key role in supporting this diversity by allowing customers to run applications in a wide variety of
operating systems and configurations.

And while the flexibility of VMs allows customers to easily move a vast array of applications into the cloud, that same flexibility inhibits the
amount of control a cloud provider can yield over VM behavior. For example, a cloud provider may be able to provide virtual networks or enforce rate limiting
on a tenant VM, but it cannot control the TCP/IP stack running on the VM. As the TCP/IP stack considerably impacts overall network performance, it
is unfortunate that cloud providers cannot exert a fine-grained level of control over one of the most important components in the networking stack.

Without having control over the VM TCP/IP stack, datacenter networks remain at the mercy of inefficient, out-dated or misconfigured TCP/IP stacks.
TCP behavior, specifically congestion control, has been widely studied and many issues have come to light when its behavior is not optimized. For example,
network congestion caused by non-optimzed stacks can lead to loss, increased latency and reduced throughput.

Thankfully, recent advances in optimizing TCP stacks for datacenter environments have shown that both high throughput and low latency can be
achieved through novel TCP congestion control algorithms. Works such as DCTCP~\cite{alizadeh2011dctcp} and TIMELY~\cite{mittal2015timely} show great promise in providing high
bandwidth and low latency by ensuring that network queues in switches do not fill up. And while these stacks are deployed in many of today's
private datacenters~\cite{singh2015jupiter,judd2015dctcp}, ensuring that a vast majority of VMs within a public datacenter will update their TCP stacks
to this new technology is a daunting, if not impossible task.

We plan to explore how operators can regain control of TCP's congestion control, regardless of the TCP stack
running in a VM. Our aim is to allow a cloud provider to utilize advanced TCP stacks, such as DCTCP, without having
control over the VM or requiring changes in network hardware. We propose implementing congestion control in the virtual switch
(vSwitch) running on each server. Implementing congestion control within a vSwitch has several advantages.
First, vSwitches naturally fit into datacenter network virtualization architectures and are widely
deployed~\cite{pfaff2015design}. Second, vSwitches can easily monitor and modify traffic passing through them.
Today vSwitch technology is mature and robust, allowing for a fast, scalable,
and highly-available framework for regaining control over the network.

Implementing congestion control within the vSwitch has numerous challenges. First, in order to ensure adoption rates are high, the
approach must work without making changes to VMs.
Hypervisor-based approaches that do not modify VMs typically rely on rate limiters to limit VM traffic. Rate limiters implemented in
commodity hardware do not scale in the number of flows and software implementations incur high CPU overhead~\cite{radhakrishnan2014senic}.
Therefore, limiting a VM's TCP flows in a fine-grained, dynamic nature
at scale (10,000's of flows per server~\cite{moshref2013vcrib}) with limited computational overhead remains challenging.
Finally, VM TCP stacks may differ in the features they support (\eg{}, ECN) or the congestion
control algorithm they implement, so a vSwitch congestion control implementation should work under a variety
of conditions.

We propose a new technology that implements
TCP congestion control within a vSwitch to help ensure VM
TCP performance cannot impact the network in an adverse way. At a high-level, the vSwitch monitors all packets for a flow, modifies
packets to support features not implemented in the VM's TCP stack (\eg{}, ECN) and reconstructs
important TCP parameters for congestion control. vSwitch runs the congestion control logic specified by an administrator and then enforces an intended
congestion window by modifying the receiver advertised window (\rwnd{}) on incoming ACKs. A policing
mechanism ensures stacks cannot benefit from ignoring~\rwnd{} and can also be used for non-TCP traffic.

\subsection{(Near) Real time network congestion monitoring}

Near real time network congestion monitoring can help the network administrator to monitor the network healthy condition. 
Such information can guide network debugging and improve network performance. 
Our network congestion monitoring scheme serves for two purposes---1) constructing a ``hot spot" map to 
aid network debugging and 2) generating fast path congestion feedback signal to the affected hosts to reroute its flows.

We plan to utilize the ECN capacities and port mirroring features that are commonly available 
in modern switches to gather congestion information. 
ECN marked packets are mirrored and sampled to the mirroring port on the switch. Each switch's 
mirroring ports is connected to a logically centralized pool of servers which run the 
congestion information analytics. The logically centralized servers have two roles. 
First they need to analyze the packets from each switches' mirroring port and figure out which 
switch (or which switch port) is congested and which (and how many) flows are 
affected by the network congestion event. Second. the centralized servers need to have a fast path to 
generate congestion feedback information to the end-hosts that are affected. 
The congestion feedback is sent through the management network (which is independent of the data network), 
so the feedback can reach the affected end-hosts in real time. Finally, 
we leverage the intelligent network edge (\eg{}, the virtual switch in the hypervisor or the smart NIC) to 
interpret the congestion feedback information and reroute the affected flows based on the 
knowledge of the centralized analytics servers. 
There are two unknown questions we need to explore---1) 
we can minimize the control loop latency to what value based on existing 
hardware and software packet processing schemes such as Intel DPDK~\cite{intel_dpdk} and Netmap~\cite{rizzo2012netmap} 
and 2) what is the scalability of the proposed approach. 

\subsection{Timeline}

Table \ref{tab:plan} shows my plan for completion of the research.

\begin{table}[hc]
\begin{small}
\begin{center}
\begin{tabular}{lll}
Timeline & Work & Progress\\
\hline
          & Presto \& SDN Latency \& CloudMeasure & completed\\
May 2016 & congestion control enforcement & proposed\\
March 2017 & network congestion monitoring  & proposed\\
May 2017 & Thesis writting & \\
June 2017 & Thesis defense & \\
\end{tabular}
\end{center}
\end{small}
\caption{Plan for completion of my research}
\label{tab:plan}
\end{table}

