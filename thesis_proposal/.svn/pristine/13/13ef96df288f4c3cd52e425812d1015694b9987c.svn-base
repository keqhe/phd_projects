\section{Introduction}
\label{ch:intro}

\subsection{Background and Motivations}
Cloud computing is changing the way computing is conducted since a few years ago.
It is a rapidly growing business and many industry leaders 
(\eg{}, Amazon~\cite{amazon-aws},
Microsoft~\cite{microsoft-azure}, IBM~\cite{ibm-softlayer,ibm-bluemix} and
Google~\cite{google-compute}) have embraced such a 
business model and are deploying highly advanced cloud computing infrastructures. 
Analysis~\cite{cloud-market2020} 
has predicted that the global cloud computing market will 
reach \$270 billion by 2020. The success of cloud computing is 
not accidental---it is rooted in many advantages that cloud computing offers 
over traditional computing model. The most notable feature is that tenants 
(customers) who rent the computing resources can get equivalent computing power 
with \emph{lower cost}. That is because the computing resources 
(CPUs, memory, storage, and network) are shared among multiple users and 
server consolidation and server virtualization improves the utilization 
of the computing resources. Another key advantage cloud computing offers 
is \emph{computing agility}. That means, tenants can rent as many computing 
resources as they needed and can grow or shrink the computing pool based on their demands. 
This feature is especially attractive for relatively smaller and 
rapidly growing businesses.

\iffalse
Building high performance, highly secure cloud computing infrastructures 
requires substantial research and engineering efforts. Lots of 
optimizations need to be done throughout the infrastructure, for example, 
server virtualization techniques, high throughput and low latency data center 
network design, highly robust and fast Internet access to cloud-hosted services, 
scalable and efficient big data analytics platforms etc. 
\fi
Datacenter network is a key enabler for high performance cloud computing infrastructures 
because network performance can affect services and tasks running in data centers significantly. 
Therefore, understanding and improving network performance for cloud-hosted services is 
an important and timely research direction.
In this proposal, 
we will research solutions that help improve cloud network performance.

\subsection{Research Questions}

%Network performance is typically characterized by throughput, latency, packet loss rate, 
%fairness and fault-tolerance. 
This proposal exams some of the most important components that affect cloud network performance. 
These components include traffic load balancing, congestion control, web server deployment
and datacenter network architecture design.
We study the current practices for each component
and explore the possibility to improve them.
More specifically, we try to answer the following questions:

\begin{enumerate}

\item How can we load balance the traffic within the data center to 
minimize the likelihood of network congestion? 
If we can (largely) eliminate network congestion, 
then cloud-hosted services' performance can be improved significantly. 

\item Who are the existing users in public clouds? 
How they are using the cloud? 
What are the deployment patterns of current web services and are they optimal?
How we can improve cloud-hosted web services' 
availability and wide area network performance?

\item In multi-tenant cloud where tenant VMs can use out-dated, inefficient, or
misconfigured TCP stacks, is it possible for administrators to 
take control of a VM's TCP congestion control
algorithm {\em without} making changes to the VM or network hardware to 
reduce network queueing latency and improve throughput fairness?

\item Can we build a datacenter network architecture analysis framework to 
compare existing datacenter network architectures 
based on metrics such as wiring complexity, bandwidth, fault-tolerance and routing convergence? 
Can we use this analysis framework to explore new datacenter network topologies and 
codesign routing, load balancing to improve robustness and performance?

\end{enumerate}

In what follows, we will present how we answer these research questions. 
We first introduce works that are completed (questions 1, 2). 
Then we will discuss some new topics we plan to work on (questions 3 and 4).

\subsection{Completed Work}
\subsubsection{Software edge-based load balancing scheme for fast data center networks}
Datacenter networks must support an increasingly diverse set of
workloads. Small latency-sensitive flows to support real-time applications such
as search, RPCs, or gaming share the network with large
throughput-sensitive flows for video, big data analytics, or VM
migration. Load balancing the network is crucial to ensure operational efficiency
and suitable application performance.
Unfortunately, popular flow-hashing-based load balancing schemes,
\eg{}, ECMP, cause congestion when hash collisions
occur~\cite{al2010hedera,dc-mptcp,rasley2014planck,zats2012detail,packetspray,cao2013drb} and
perform poorly in asymmetric topologies~\cite{alizadeh2014conga,zhou2014wcmp}.
Conceptually, ECMP's flaws are not internal to its operation but are caused by 
asymmetry in network topology (or capacities) and variation in flow sizes. 
{\em In a symmetric network topology
where all flows are ``mice'', ECMP should provide near optimal load balancing}; 
indeed, prior work~\cite{alizadeh2014conga,flowlet} has shown the traffic imbalance 
ECMP imposes across links goes down with an increase in the number of flows and 
a reduction in the variance of the flow size distribution.

We leverage this insight to design a high performance proactive load balancing 
scheme without requiring 
special purpose hardware or modifications to end-point transport.
It relies on the datacenter network's {\em software edge} to 
transform arbitrary sized flows into a large number of near uniformly sized small sub-flows, 
and to proactively spread those uniform data units over the network in a balanced fashion. 
The proposed scheme is fast (works at 10+ Gbps), and doesn't require network stack 
configurations that 
may not be widely supported outside the datacenter (such as increasing MTU sizes). 
It piggybacks on recent trends where several network functions, \eg{}, 
firewalls and application-level load balancers, are moving into hypervisors and 
software virtual switches on end-hosts~\cite{koponen2014network,pfaff2015design,pfaff2009extending}. 
Several challenges arise when employing the edge to load balance the network
on a sub-flow level. Software is slower than hardware, so operating at 10+ Gbps speeds means
algorithms must be simple, light-weight, and take advantage of optimizations in
the networking stack and offload features in the NIC. Any sub-flow level load balancing should also be
robust against reordering.
Reordering not only impacts TCP's congestion control mechanism, 
but also imposes significant computational
strain on hosts, effectively limiting TCP's achievable bandwidth if not properly 
controlled (see \secref{sec:presto-background}).
Last, the approach must be resilient to hardware or link failures and be adaptive to network asymmetry.

We build a proactive load balancing system called Presto to solve the challenges 
mentioned above.
Presto utilizes vSwitches to break flows into discrete units of packets, called
{\em flowcells}, and distributes them evenly
to near-optimally load balance the network.
Presto uses the maximum TCP Segment Offload (TSO) size (64 KB) as flowcell granularity,
allowing for fine-grained load balancing at network speeds of 10+ Gbps.
To combat reordering, we modify the Generic Receive Offload (GRO) handler
in the hypervisor OS to mitigate the computational burden imposed by reordering
and prevent reordered packets from being pushed up the networking stack.
Finally, we show Presto can load balance the network
in the face of asymmetry and failures using a combination of fast failover and 
weighted multipathing at the network edge.
We evaluate Presto on a real 10 Gbps testbed. Our experiments show Presto
outperforms existing load balancing schemes (including flowlet switching, ECMP, MPTCP) and
is able to track the performance of a single, non-blocking switch (an optimal case) within a few percentage points
over a variety of workloads, including trace-driven. Presto improves throughput, latency and fairness in the network and
also reduces the flow completion time tail for mice flows. This work is published in ACM 
SIGCOMM'15~\cite{he2015presto}.

\subsubsection{Characterizing web service deployment in public clouds} 

Today, web services are
increasingly being deployed in infrastructure-as-a-service (IaaS) clouds such
as Amazon EC2, Windows Azure, IBM SoftLayer and Rackspace. Industry and the media claim
that over 1\% of Internet traffic goes to EC2~\cite{wiredarticle} and that outages in
EC2 are reputed to hamper a huge variety of
services~\cite{AWSoutage2011,AWSoutageOct2012,netflixoutage,wsjarticle}.
Despite the popularity of public IaaS clouds, we are unaware of any in-depth
measurement study exploring the current usage patterns of these environments.
Prior measurement studies have either quantified the compute, storage, and
network performance these clouds
deliver~\cite{li2010cloudcmp,li2011cloudprophet}, evaluated the
performance and usage patterns of specific services that are hosted in these
clouds, e.g., Dropbox~\cite{drago2012inside}, or examined cloud usage solely
in terms of traffic volume~\cite{deepfield}.

We present the first in-depth empirical study of modern IaaS clouds that
examines {\em IaaS cloud usage patterns} and identifies {\em ways in which
cloud tenants could better leverage IaaS clouds to improve performance}. Our focus is particularly
on web services hosted within IaaS clouds, which our study (unsurprisingly)
indicates is a large and important usage case for IaaS.
We first examine {\em who is using public IaaS clouds}. We generate a dataset
of cloud-using domains using extensive DNS probing in order to compare the IPs
associated with websites on Alexa's top 1 million list~\cite{alex_topdomains}
against published lists of cloud IP ranges.  This identifies
that $\approx$40K
popular domains (4\% of the Alexa top million) have a subdomain
running atop Amazon EC2 or Windows Azure, two of the largest public clouds.
We extract an additional $\approx$13K cloud-using domains from a full
packet capture taken at the edge of our university, and use this capture
to characterize the network traffic patterns of cloud-hosted web services.
These results indicate that a large fraction of important web services are already
hosted within public IaaS clouds.

Further, we dissect {\em how these services are using the
  cloud}. EC2 and Azure both have a veritable potpourri of features,
including virtual machines, load balancers, platform-as-a-service
(PaaS) environments, content-distribution networks (CDNs), and domain
name services. They also give tenants the choice of deploying their
services in several different regions (i.e., geographically distinct
data centers), and EC2 provides several different ``availability
zones'' within each region. We couple analysis of DNS records with
two different cloud cartography techniques~\cite{ristenpart2009hey} to
identify which features, regions and zones web services use. We
identify several common \frontend deployment patterns and report
estimates of the percentages of Alexa subdomains using each of the
patterns. In particular, we find that about 4\% of
EC2-using web services use load balancers and 8\% of them
make use of PaaS. 
We also show that 97\% of subdomains hosted on EC2 and 92\% of
subdomains hosted on Azure are deployed in only a single region.
Counted among these are the subdomains of most of the top 10 (by Alexa
rank) cloud-using domains. Services deployed in EC2 also appear to
make limited use of different availability zones: our measurements
estimate that only 66\% of subdomains use more than one zone and only
22\% use more than two. This lack of redundancy means that many (even
highly ranked Alexa) services will not tolerate single-region or even
single-zone failures.

Finally, we use a series of PlanetLab-based~\cite{planetlab} active measurements and
simulations to {\em estimate the impact of wide-area route outages and
  the potential for wide-area performance improvement}. We find that
expanding a deployment from one region to three could yield 33\% lower
average latency experienced by globally distributed clients, while
also substantially reducing the risk of service downtime due to
downstream Internet routing failures.
This work is published in ACM IMC'13~\cite{he2013next}.

\subsection{Proposed Work}
\subsubsection{Congestion control enforcement for improved data center networks}
\iffalse
DCTCP~\cite{alizadeh2011dctcp} is a drop-in congestion control replacement for TCP that aims to keep 
switch buffers low in the network. Companies such as Morgan Stanley have 
instituted DCTCP in their datacenter~\cite{judd2015dctcp}. 
DCTCP, however, requires changes to the TCP stack. 
In public data centers, providers may not have control over the transport stack. 
In other words, cloud providers can not enforce the transport layer used by customers, 
for example tenants can use whatever version of TCP (either CUBIC, New Reno or DCTCP) in 
Amazon's dedicated instances or VMs, IBM's bare-metal machines etc. 

So is there any way we can achieve DCTCP-like properties in the network 
(either in the core or on the soft edge, like Open vSwitch) even for the stacks we can not control? 
If the overlying TCP stack can be inferred, perhaps we can play some games in the network to 
bias its congestion window. For example, we could rate-limit packets at the edge according to 
a more conservative congestion window, or we could modify the receiver's 
advertised window size on incoming TCP packets to enforce the sender to slow its rate. 
\fi
Multi-tenant datacenters are successful because tenants
can seamlessly port their workloads, applications and services
to the cloud. Virtual Machine (VM) technology plays an integral role in this success
by enabling a diverse set of operating systems and
software to be run on a unified underlying framework. This
flexibility, however, comes at the cost of dealing with out-dated, inefficient, or
misconfigured TCP stacks implemented in the VMs. We investigate if
administrators can take control of a VM's TCP congestion control
algorithm {\em without} making changes to the VM or network hardware.
We propose a virtual switch-based congestion control enforcement scheme that exerts fine-grained
control over arbitrary tenant TCP stacks by enforcing per-flow congestion
control in the vSwitch. Our scheme is light-weight, flexible, scalable and
can police non-conforming flows.

\iffalse
Our goal is not limited to just keeping switch buffers low in the network 
such that we can achieve low latency for the whole data center network. 
As pointed out by~\cite{judd2015dctcp}, when TCP traffic is mixed with DCTCP traffic and ECN marking 
is turned on, there are two practical performance issues: 1)DCTCP can gain almost all 
the bandwidth while TCP can only get very little share and 2)TCP or DCTCP's connection 
establishment probability is decreased sharply with the increase of the number of 
competing flows in the network. All these two issues are related to the switch's 
AQM (Active Queue Management) scheme---it simply drops non-ECT (ECN-Capable Transport) 
packets when the switch's queue length is (even slightly) larger than the threshold. 
One of the solutions is to apply ECT to all packets at the virtualization edge 
(no matter it is TCP CUBIC or DCTCP, no matter it is control packets or data packets). 
We undo the ECT marking properly when the packet arrives the destination host. 
Finally, we also hope that our congestion control enforcement 
logic is beneficial to the lossness \emph{ Data Center Bridging } or \emph{ Converged Ethernet } 
environment where there are serious bufferbloat (hence increase network latency) and 
fairness issues~\cite{tcp-bolt,zhu2015rdma}.
\fi

\subsubsection{Datacenter network architecture analysis and exploration}
Although there have been many datacenter network topologies 
proposed~\cite{fattree,niranjan2009portland,vl2,singh2015jupiter}, 
to date, there are very few datacenter architecture analysis frameworks 
that help network architect evaluate different network topologies 
at large scale. Hence, we propose to  build a network architecture analysis 
framework that helps to compare different network architectures based on bisection 
bandwidth, latency, fault tolerance and routing convergence time and similar metrics. 
Based on this analysis framework, we will investigate if we can identify new topologies 
that can improve the current practices and codesign routing, load balancing and network topology.

\subsection{Structure of Thesis Proposal}
\label{outline}
The rest of this proposal is organized as follows: Section~\ref{related} discusses related work.
Section~\ref{presto} discusses the edge-based traffic load balancing scheme for data center networks. 
Section~\ref{cloudmeasure} discusses our work 
on characterizing web service deployment in public cloud, our observations and 
suggestions to improve network performance and robustness. 
Section~\ref{plan} presents two new research topics I plan to work on and 
gives an estimated timeline to finish the rest part of my thesis.

Section~\ref{presto} and Section~\ref{cloudmeasure} are based on previous publications~\cite{he2015presto,he2013next}.
