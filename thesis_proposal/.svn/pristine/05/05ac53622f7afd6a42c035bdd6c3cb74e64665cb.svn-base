
\begin{figure*}[!tb]
	\centering
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
		\includegraphics[width=\linewidth]{./figures/mazu/jan27_bcm_add_same_burst_100-eps-converted-to.pdf}
		\caption{burst size 100, same priority}
		\label{fig:bcm_burst_100_same_pri}
	\end{subfigure}
	\centering
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figures/mazu/jan27_bcm_add_same_burst_200-eps-converted-to.pdf}
		\caption{burst size 200, same priority}
		\label{fig:bcm_burst_200_same_pri}
	\end{subfigure}
        \centering
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figures/mazu/jan27_bcm_add_incr_burst_100-eps-converted-to.pdf}
		\caption{burst size 100, incr. priority}
		\label{fig:bcm_burst_100_incr_pri}
	\end{subfigure}
        \centering
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figures/mazu/jan27_bcm_add_incr_burst_200-eps-converted-to.pdf}
		\caption{burst size 200, incr. priority}
		\label{fig:bcm_burst_200_incr_pri}
	\end{subfigure}
	\caption{{\bf \BroadcomOne} priority per-rule {\bf insert} latency}
	\label{fig:priority-broadcom-insert}
\end{figure*}


{\emph {We first examine how different rule workloads impact insertion latency}}. We
insert a burst of $B$ rules: $r_1,\cdots,r_B$. Let $T(r_i)$ be the time we
observe the first packet matching $r_i$ emerging from the output port
specified in the rule. We define per-rule insertion latency as $T(r_i)-T(r_{i-1})$.  

\tightparagraph{Rule Complexity} 
To understand the impact of rule complexity (i.e., the number of header 
fields specified in a rule), we install bursts of rules that specify either
2, 8, or 12 fields. In particular, we specify destination IP and EtherType
(others wilcarded) in the 2-field case; input port, EtherType, source and
destination IPs, ToS, protocol, and source and destination ports in the
8-field case; and all supported header fields in the 12-field (exact match)
case. We use a burst size of 100 and all rules have the same priority.
We find that rule complexity {\em does not} impact insertion latency. The
mean per-rule insertion delay for 2-field, 8-field, and exact
match cases is 3.31ms, 3.44ms, and 3.26ms, respectively, for \BroadcomOne.
Similarly, the mean per-rule insertion delay for \Intel, \IBM, and
\BroadcomThree is $\approx$ 1 ms irrespective of the number of fields. 
All experiments that follow use rules with 2 fields.

\tightparagraph{Table occupancy} To understand the impact of table occupancy, we
insert a burst of $B$ rules into a switch that already has $S$ rules
installed. All $B+S$ rules have the same priority. We fix $B$ and
vary $S$, ensuring $B+S$ rules can be accommodated in each switch's hardware
table.
We find that flow table occupancy {\em
does not} impact insertion delay if all rules have the same priority.
Taking $B=400$ as an example, the mean per-rule insertion delay is 3.14ms, 
1.09ms, 1.12ms, and 1.11ms (standard deviation 2.14ms, 1.24ms, 1.53ms, and
        0.18ms) for \BroadcomOne, \BroadcomThree, \IBM
and \Intel, respectively, regardless of the value of $S$. 

\tightparagraph{Rule priority} To understand the effect of rule priority on the
insertion operations, we conducted three different experiments each covering
different patterns of priorities. In each, we insert a burst of $B$ rules
into an empty table ($S=0$); we vary $B$. In the {\em same priority}
experiment, all rules have the same priority. In the {\em increasing} and
{\em decreasing priority} experiments, each rule has a different priority and
the rules are inserted in increasing/decreasing priority order, respectively. 

Representative results for same priority rules are shown in 
\figsref{fig:bcm_burst_100_same_pri}{fig:bcm_burst_200_same_pri} for
$B=100$ and $B=200$, respectively; the switch is \BroadcomOne. For both
burst sizes, the per-rule insertion delay is similar, with medians of 3.12ms
and 3.02ms, and standard deviations of 1.70ms and 2.60ms for $B=100$ and
$B=200$, respectively. The same priority insertion delays on \BroadcomThree,
\IBM, and \Intel are slightly lower, but still similar: mean per-rule
insertion delay is 1.09ms, 1.1ms, and 1.17ms, respectively, for $B=100$.  We
conclude that {\em same priority} rule insertion delay does not vary with burst
size.

In contrast, the per-rule insertion delay of increasing priority rules 
{\em increases linearly} with the number of rules inserted for \BroadcomOne,
\BroadcomThree, and \IBM.
\figsref{fig:bcm_burst_100_incr_pri}{fig:bcm_burst_200_incr_pri} shows this
effect for $B=100$ and $B=200$, respectively, for \BroadcomOne.  Compared with
the same priority experiment, the average per-rule delay is much larger:
9.47ms (17.66ms) vs. 3.12ms (3.02ms), for $B=100$ (200). The results are
similar for \BroadcomThree and \IBM: the average per-rule insertion delay is
7.75ms (16.81ms) and 10.14ms (18.63) for $B=100$ (200), respectively. 
We also observe the slope of the latency increase is constant---for a given
switch---regardless of $B$.

The increasing latency in \BroadcomOne, \BroadcomThree, and \IBM stems from
the TCAM storing high priority rules at low (preferred) memory addresses. Each
rule inserted in the {\em increasing priority} experiments displaces all prior
rules!
Surprisingly, latency does not increase when increasing priority rules are
inserted in \Intel. As shown in \figref{fig:intel_burst_800_incr_pri}, the
median per-rule insertion delay for \Intel is 1.18ms (standard deviation of
1.08ms), even with $B=800$! Results for other values of $B$ are similar.
This shows that the \Intel TCAM architecture is fundamentally different from
\Broadcom and \IBM. Rules are ordered in \Intel's TCAM such that higher
priority rules do not displace existing low priority rules. 
However, displacement does still occur in \Intel. 
\figref{fig:intel_burst_800_decr_pri} shows per-rule insertion latencies for
for {\em decreasing priority} rules for $B=800$. We see two effects: (1) the
latencies alternate between two modes at any given time, and (2) there is a
step-function effect after every 300 or so rules. 
A likely explanation for the former is bus buffering. Since rule insertion is
part of the switch's control path, it is not really optimized for latency.
The latter effect can be explained as follows: Examining the \Intel switch
architecture, we find that it has 24 slices, $A_1\ldots A_{24}$, and each
slice holds 300 flow entries. There exists a consumption order (low-priority
first) across all slices.  Slice $A_i$ stores the $i^{th}$ lowest
priority rule group. If rules are inserted in decreasing priority, $A_1$ is
consumed first until it becomes full. When the next low priority rule is
inserted, this causes one rule to be displaced from $A_1$ to $A_2$.  This
happens for each of the next 300 rules, after which cascaded displacements
happen: $A_1 \rightarrow A_2 \rightarrow A_3$, and so on. We confirmed this
with \Intel.

We observe different trends when inserting {\em decreasing priority} rules
in \BroadcomOne, \BroadcomThree, and \Intel. With \BroadcomOne, we find the
average per-rule insertion delay increases with burst size: 8.19ms for $B=100$
vs. 15.5ms for $B=200$. Furthermore, we observe that the burst of $B$ rules is
divided into several groups, and each group is reordered and inserted in the
TCAM in order of increasing priority. This indicates that \BroadcomOne
firmware reorders the rules and prefers increasing priority insertion.  In
contrast, \BroadcomThree's per-rule insertion delay for decreasing priority
rules is similar to same priority rule insertion: $\approx$ 1ms. Hence, the
\BroadcomThree firmware has been better optimized to handle decreasing
priority rule insertions. The same applies to \Intel: per-rule insertion delay
for decreasing priority rules is similar to same priority rule insertion:
$\approx$ 1.1ms.


\begin{figure*}[!tb]
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figures/mazu/jan27_intel_incr_burst_800-eps-converted-to.pdf}
		\caption{burst size 800, incr. priority}
		\label{fig:intel_burst_800_incr_pri}
	\end{subfigure}
        \begin{subfigure}[b]{0.40\textwidth}
                \centering
		\includegraphics[width=\textwidth]{figures/mazu/jan27_intel_empty_800L_decr_delta-eps-converted-to.pdf}
		\caption{burst size 800, decr. priority}
		\label{fig:intel_burst_800_decr_pri}
	\end{subfigure}
	\caption{{\bf Intel} priority per-rule {\bf insert}} 
	\label{fig:priority-intel-insert}
\end{figure*}


\tightparagraph{Priority and table occupancy combined effects} 
We now study the combined impact of rule priority and table occupancy.
We conduct two experiments: For the first experiment, the table starts with
$S$ high priority rules, and we insert $B$ low priority rules.  For the
second experiment, the priorities are inverted.
For both experiments, we measure the total time to install all rules in the
burst, $T(r_B)-T(r_1)$.

\begin{figure*}[!t]
	\centering
        \begin{subfigure}[b]{0.40\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figures/mazu/bcm_two_pri_high_low_burstB-eps-converted-to.pdf}
		\caption{insert low priority rules into a table with high priority rules}
		\label{fig:bcm_outbound_two_pri_high_low_burstB}
	\end{subfigure}
        \begin{subfigure}[b]{0.40\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figures/mazu/bcm_two_pri_low_high_burstB-eps-converted-to.pdf}
		\caption{insert high priority rules into a table with low priority rules}
		\label{fig:bcm_outbound_two_pri_low_high_burstB}
	\end{subfigure}
	\caption{Overall completion time on {\bf \BroadcomOne}.  Initial table occupancy is S high (low) priority rules; insert a burst of low (high) priority rules. Averaged over 5 runs. }
	\label{fig:burst-completion-time}
\end{figure*}


For \BroadcomOne, \BroadcomThree, and \IBM, we expect that as long as the same
number of rules are displaced, the completion time for different values of
$S$ should be the same.
Indeed, from \figref{fig:bcm_outbound_two_pri_high_low_burstB} (for
\BroadcomOne), we see that even with 400 high priority rules in the
table, the insertion delay for the first experiment is no different from the
setting with only 100 high priority rules in the table. In contrast, in
\figref{fig:bcm_outbound_two_pri_low_high_burstB}, newly inserted high
priority rules will displace low priority rules in the table, so when
$S=400$ the completion time is about 3x higher than $S=100$.
%\fixme{added IBM below}
For \IBM (not shown), inserting 300 high priority rules into a table with 400
low priority rules takes more than 20 seconds.  


%For \Intel, we also run the same two experiments as for Broadcom. 
For \Intel, the results are similar to same priority rule insertion. This
indicates that \Intel uses different TCAM organization schemes than the
\Broadcom and \IBM switches.  %\fixme{changed}
%optimizes for rule priority better than \BroadcomOne. 
 
\tightparagraph{Summary and root causes}
We observe that: (1) rule complexity does not affect insertion delay; (2)
same priority insertions in \BroadcomOne, \BroadcomThree, \Intel and \IBM are fast
and not affected by flow table occupancy; and (3) priority insertion patterns
can affect insertion delay very differently. For \Intel, increasing priority
insertion is similar to same priority insertion, but decreasing priority 
incurs much higher delay. For \BroadcomThree and \IBM the behavior is inverted:  
decreasing priority insertion is similar to same priority insertion and increasing priority insertion incurs higher delay. For \BroadcomOne, 
insertions with different priority patterns are all much higher than
insertions with same priority. 
Key root causes for observed latencies are: (1) how rules are organized in the TCAM, and (2) the number of slices. {\em Both of these are intrinsically tied to switch hardware.} Even in the best case (\Intel), per-rule insertion latency of 1ms is higher than what native TCAM hardware can support (100M updates/s~\cite{estan:private}). Thus, in addition to the above two causes, there appears to be an {\em intrinsic switch software overhead} contributing to all latencies.

