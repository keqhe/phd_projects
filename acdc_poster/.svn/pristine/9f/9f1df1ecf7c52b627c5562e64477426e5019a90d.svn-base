\section{Introduction}
\label{section:intro}
Multi-tenant datacenters are successful because tenants
can seamlessly port their workloads, applications and services
to the cloud. Virtual Machine (VM) technology plays an integral role in this success by enabling a diverse set of operating systems and
software to be run on a unified underlying framework. This
flexibility, however, comes at the cost of dealing with out-dated, inefficient, or misconfigured TCP stacks implemented in the VMs. This paper investigates if administrators can take control of a VM's TCP congestion control
algorithm without making changes to the VM or network hardware.
We propose a scheme that exerts fine-grained
control over arbitrary tenant TCP stacks by enforcing per-flow congestion
control in the vSwitch. 
Our scheme is light-weight, flexible, scalable and can police non-conforming flows. 
%Our evaluation shows the computational overhead of LiquidSwitch is less than 4\% and implementing an administrator-defined congestion control algorithm
%in the vSwitch (\ie{}, DCTCP) closely tracks its native performance, regardless of the VM's TCP stack.


\section{Our Approach}
\iffalse
\begin{figure}[!t]
        \centering
  \includegraphics[width=0.45\textwidth]{figures/acdc_highlevel.pdf}
        \caption{\acdc{} high-level architecture.}
        \label{acdc_highlevel}
\end{figure}
\fi
%Figure~\ref{acdc_highlevel} shows the high-level structure of \acdc{}.
We present \acdc{}, a new technology that implements
TCP congestion control within a vSwitch to help ensure VM
TCP performance cannot impact the network in an adverse way. At a high-level, the vSwitch monitors all packets for a flow, modifies
packets to support features not implemented in the VM's TCP stack (\eg{}, ECN) and reconstructs
important TCP parameters for congestion control.~\acdc runs the congestion control logic specified by an administrator and then enforces an intended
congestion window by modifying the receiver advertised window (\rwnd{}) on incoming ACKs. A policing
mechanism ensures stacks cannot benefit from ignoring~\rwnd{} and can also be used for non-TCP traffic.
\begin{figure}[!t]
        \centering
  \includegraphics[width=0.35\textwidth]{figures/acdc_highlevel_long.pdf}
        \caption{\acdc{} high-level architecture.}
        \label{acdc_highlevel}
\end{figure}

Our scheme provides the following benefits. First,~\acdc allows for
administrators to enforce a uniform, network-wide congestion control algorithm without changing VMs. When using congestion control algorithms tuned for
datacenters (\ie{}, DCTCP~\cite{alizadeh2011data}), this allows for high throughput and low latency, regardless of the congestion control algorithms VMs use. Second,
our system mitigates the impact of varying TCP stacks running on the same fabric. This improves fairness and additionally
solves the ECN co-existence problem identified in production networks~\cite{wu2012tuning,judd2015nsdi}.
Third, our scheme is easy to implement, computationally lightweight, scalable and modular.
% so that it is highly complimentary to
%performance isolation schemes also designed for virtualized datacenter environments.

\section{Experiment Results}
\iffalse
\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./figures/macro/stride/macro_compare_tput_witherrbar.pdf}
                \caption{}
                \label{tput}
        \end{subfigure}
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./figures/macro/bijection/macro_compare_fct_bijection_mice.pdf}
                \caption{}
                \label{fct}
        \end{subfigure}
        \caption{System performance (a) elephant flow throughput and (b) mice flow (50KB) completion time.}
        \label{performance}
\end{figure}
\fi

Our evaluation shows the computational overhead of \acdc{} is less than 4\% and implementing an administrator-defined congestion control algorithm
in the vSwitch (\ie{}, DCTCP) closely tracks its native performance, regardless of the VM's TCP stack.
