\section{Introduction}
\label{intro}

Multi-tenant datacenters are a crucial component of today's computing ecosystem. Large providers, such as Amazon, Microsoft, IBM, Google and Rackspace, support
a diverse set of customers, applications and systems through their public cloud offerings. These offerings are successful in 
part because they can provide efficient performance to a wide-class of applications running on a diverse set of platforms. Virtual
Machines (VMs) play a key role in supporting this diversity by allowing customers to run applications in a wide variety of 
operating systems and configurations.

And while the flexibility of VMs allows customers to easily move a vast array of applications into the cloud, that same flexibility inhibits the 
amount of control a cloud provider can yield over VM behavior. For example, a cloud provider may be able to provide virtual networks or enforce rate-limiting
on a tenant VM, but it cannot control the TCP/IP stack running on the VM. As the TCP/IP stack considerably impacts overall network performance, it 
is interesting that cloud providers cannot exert a fine-grained level of control over one of the most important components in the networking stack.

Without having control over the VM TCP/IP stack, datacenter networks remain at the mercy of inefficient, out-dated or misconfigured TCP/IP stacks.
TCP behavior, specifically congestion control, has been widely studied and many issues have come to light when its behavior is not optimized. For example,
network congestion caused by non-optimzed stacks can lead to loss, increased latency and reduced throughput. As revenue increasingly is tied to 
strict latency and bandwidth requirements from workloads such as big data analytics and search (~\todo{cite}), public cloud providers must ensure their
network fabrics can provide tight service-level agreements and deadlines required by customer applications.

Thankfully, recent advances in optimizing TCP stacks for datacenter environments have shown that both high throughput and low latency can be 
achieved through novel TCP congestion control algorithms. Works such as DCTCP~\cite{alizadeh2011data} and TIMELY~\cite{mittal2015timely} show great promise in providing high
bandwidth and low latency by ensuring that network queues in switches do not fill up. And while these stacks are deployed in many of today's 
private datacenters~\cite{singh2015jupiter,judd2015nsdi}, ensuring that a vast majority of VMs within a public datacenter will update their TCP stacks
to this new technology is a daunting, if not impossible task.

In this paper, we explore how operators can regain control of TCP's congestion control, regardless of whether or not the TCP stack
is running in a VM. Our aim is to allow a public cloud provider to utilize advanced TCP stacks, such as DCTCP, without having
any control on the VM or requiring any changes in network hardware. We propose implementing congestion control in the virtual switch
(vSwitch) running on each server. Implementing congestion control within a vSwitch has several advantages. 
First, vSwitches naturally fit into current datacenter network virtualization architectures~\cite{Pfaff2015ovs} and are widely
deployed. Second, vSwitches can easily monitor and modify all traffic passing through them. 
Today vSwitch technology is mature and robust, allowing for a fast, scalable,
and highly-available framework for regaining control over the network. 

%Since vSwitch technology supports software-defined networking (say OVS),
%implementing congestion control within the vSwitch can also naturally support advanced congestion control algorithms, such as centralized 
%or proactive schemes (cite). 

Implementing congestion control within the vSwitch has numerous challenges, however. First, in order to ensure adoption rates are high, the 
approach must work without making changes to VMs. Hypervisor-based approaches to limit the rate at which a TCP flow sends typically are implemented in hardware (which doesn't 
scale) or software (which incurs high CPU overhead). Therefore, limiting the rate of a VM's TCP stack in a fine-grained, dynamic nature
at scale (10,000's of flows) with limited computational overhead remains challenging. Finally, VM TCP stacks may differ in the features they support (ECN) or the congestion
control algorithm they implement (delay-based and loss-based), and a vSwitch congestion control implemention should work under these
different conditions. 

In this paper, we present Alternative Congestion for Datacenter TCP (\acdc{} TCP, or simply~\acdc{}), a new technology that implements 
TCP congestion control within a vSwitch in order to ensure VM
TCP performance cannot impact the network in an adverse way. At a high-level, the vSwitch monitors all packets for a flow, modifies transient
packets to support features not implemented in the VM's TCP stack (like ECN) and reconstructs
important TCP parameters for congestion control.~\acdc runs the congestion control logic specified by administrator and then enforces an intended
congestion window by modifying the receiver advertised window (\rwnd{}) on incoming ACKs. 

Our scheme provides the following benefits. First,~\acdc allows for 
administrators to enforce a unified, network-wide congestion control algorithm without changing VMs. When using congestion control algorithms tuned for 
datacenter environments, this allows for high throughput and low latency, regardless of congestion control algorithms VMs use. Second,
our system mitigates the impact of varying TCP stacks running on the same fabric. This allows for improved TCP fairness within the network, and additionally
allows for the co-existence of both ECN and non-ECN capable flows in a seamless manner\todo{seems that ECT and non-ECT coexistence is more important than TCP fairness}. 
Third, our scheme is easy to implement, computationally lightweight, scalable, and modular so that it is highly complimentary to network-based
performance isolation schemes.

The contributions of this paper are as follows:
\begin{enumerate}
\item The design of a vSwitch-based congestion control mechanism that regains control over the TCP/IP stack implemented in
the VM without requiring any changes to the VM or network hardware. 
\item Prototype implementation to show our scheme is effective, scalable, simple to implement and has only 4\% computational
overhead over a baseline scheme.~\todo{We also provide a simple policing scheme to see if tenants are following rules. Add?} 
\item A large set of results, mostly focusing on a case study of implementing DCTCP within the vSwitch. We show~\acdc 
can track the performance of DCTCP extremely closely in many scenarios~\todo{XXX say some numbers here?}, signifcantly
reducing the queueing latency in network switches.
\end{enumerate}

The outline of the paper is as follows. Background and motivation in Section~\ref{background}. Design in Section~\ref{design}. Implementation
and Results in Sections~\ref{impl} and~\ref{results}, respectively. Related work in Section~\ref{related}. Discussion in Section~\ref{discuss}. And finally conclude
in Section~\todo{XXX}. 

