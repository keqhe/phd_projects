\section{Results}
\tightparagraph{Testbed}
The experiments are conducted on a physical testbed with IBM System x3620 M3 servers (6-core Intel Xeon
2.53GHz CPUs, 60GB memory) and Mellanox ConnectX-2 EN 10GbE NICs.
Our switches are IBM G8264 (Broadcom Trident+ inside) switches, each with a buffer of 9MB shared
by 48 10G ports and 4 40G ports.
The switches have dynamic memory management enabled by default.

\tightparagraph{System settings}
We run Linux kernel 3.18.0 which implements DCTCP as a pluggable congestion control module~\cite{linux-dctcp}.
We use either CUBIC (the default in Linux)~\cite{ha2008cubic} or DCTCP as the congestion control algorithm unless otherwise specified.
We set {{\tt $RTO_{min}$} to 10 milliseconds~\cite{vasudevan2009safe,judd2015nsdi}.
We also set {\tt tcp\_low\_latency}, {\tt tcp\_no\_metrics\_save}, {\tt tcp\_sack} to 1 and
keep other congestion control related parameters as default.
Our \acdc{} is implemented based on Open vSwitch v2.3.2~\cite{ovs-website}.
We run our experiments with either standard Open vSwitch or \acdc{}.
The baseline we compare with is termed ``Default'' where no WRED/ECN is set on 
the switches and standard Open vSwitch was running.
We use MTU size 9KB unless otherwise noted.

\tightparagraph{Performance metrics}
In \cref{micro} and \cref{incast},
the performance metrics we use are: TCP RTT (measured by sockperf~\cite{sockperf}),
TCP throughput (measured by iperf),
packet loss rate (by collecting switch counters) and
Jain's fairness index~\cite{jain-index}.
In \cref{macro}, we use flow completion time (FCT)~\cite{dukkipati2006flow} to quantify application level performance.

\input{microbenchmarks}
\input{incast}
\input{macrobenchmarks}
