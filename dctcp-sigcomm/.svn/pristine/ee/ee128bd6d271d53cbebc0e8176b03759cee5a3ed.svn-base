\section{Design}
\label{design}

\begin{algorithm}[t]
\caption{ECT marking in \acdc{}}
\label{alg:non-ect-to-ect}
\begin{algorithmic}[1]
%\STATE \Comment{packet $p$ is TCP packet}
\IF{$p$ is going to the network} 
\IF{$p$ is non-ECT}
\STATE change $p$ to ECT by modifying IP header
\STATE set $p.pretendECT$
%\STATE set the 2nd bit of the 3 reserved bits in TCP header to 1 
\ENDIF
\ELSIF{$p$ is coming from the network}
\IF{$p$ is non-ECT and $p.pretendECT$ is set}
\STATE change $p$ back to non-ECT by modifying IP header
\STATE clear $p.pretendECT$
%\STATE clear the 2nd bit of the reserved bits 
\ENDIF
\ENDIF
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Carrying the congestion information back}
\label{alg:outgoing}
\begin{algorithmic}[1]
%\STATE flow\_key $\leftarrow$ \{dstip, srcip, dstport, srcport\}
%\STATE ack\_entry $\leftarrow$ rcv\_ack\_hashtbl\_lookup(flow\_key)
%\STATE end\_seq $\leftarrow$ ntohl(tcp.seq) + tcp.data\_len
%\IF{after(end\_seq, ack\_entry.snd\_nxt)}
%	\STATE ack\_entry.snd\_nxt $\leftarrow$ end\_seq
%\ENDIF
%\IF{tcp.ack}
\IF{outgoing skb is a TCP ACK}
\STATE flow\_key $\leftarrow$ \{dstip, srcip, dstport, srcport\}
\STATE entry $\leftarrow$ rcv\_data\_hashtbl\_lookup(flow\_key)
	\IF{entry.total\_bytes > 0}
		\STATE ecn\_bytes $\leftarrow$ entry.ecn\_bytes
		\STATE total\_bytes $\leftarrow$ entry.total\_bytes
		\STATE entry.ecn\_bytes $\leftarrow$ 0
		\STATE entry.total\_bytes $\leftarrow$ 0
	\ENDIF
	\IF{total\_bytes > 0}
		\IF{skb.data\_len < (MTU -- HEADROOM)}
			\STATE skb $\leftarrow$ ovs\_pack(skb, ecn\_bytes, total\_bytes)
		\ELSE
			\STATE fack $\leftarrow$ ovs\_fack(skb, ecn\_bytes, total\_bytes)	
		\ENDIF
	\ENDIF
\ENDIF
\IF{fack != NULL}
	\STATE sendtoNIC(fack)
\ENDIF
\STATE sendtoNIC(skb)
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Congestion control in \acdc{}}
\label{alg:incoming}
\begin{algorithmic}[1]
\IF{incoming skb is a TCP ACK}
\STATE initialize is\_rack, is\_pack and is\_fack
\STATE flow\_key $\leftarrow$ \{srcip, dstip, srcport, dstport\}
\STATE entry $\leftarrow$ rcv\_ack\_hashtbl\_lookup(flow\_key)
\IF{is\_pack or is\_fack}
	\STATE extract or unpack ecn\_bytes and total\_bytes
	\STATE update(entry.ecn\_bytes, ecn\_bytes)
	\STATE update(entry.total\_bytes, total\_bytes)
\ENDIF
\IF{is\_rack or is\_pack}
	\STATE acked $\leftarrow$ tcp.ack\_seq -- entry.snd\_una
	\STATE entry.snd\_una $\leftarrow$ tcp.ack\_seq
	\IF{duplicated ACK}
		\STATE entry.dupack\_cnt++
	\ENDIF
	\STATE dctcp\_update\_alpha(entry)
	\IF{acked > 0}
		\IF{may\_raise\_rwnd(entry)}
			\STATE tcp\_cong\_avoid(entry, acked)
		\ENDIF
	\ENDIF
\ENDIF
\IF{entry.dupack\_cnt >= 3 or ecn\_bytes > 0}
	\IF{entry.dupack\_cnt >= 3}
        	\STATE entry.alpha $\leftarrow$ MAX\_ALPHA
                \STATE entry.loss $\leftarrow$ true
        \ENDIF
	\IF{may\_reduce\_rwnd(entry)}
		\STATE entry.ssthresh $\leftarrow$ dctcp\_ssthresh(ack\_entry)
		\STATE entry.rwnd $\leftarrow$ min(RWND\_MIN, entry.ssthresh)
		\STATE entry.reduced $\leftarrow$ true
	\ENDIF
\ENDIF
\IF{is\_rack or is\_pack}
	\STATE tcp.window $\leftarrow$ min(tcp.window, entry.rwnd)
\ENDIF
\IF{is\_fack}
	\STATE drop(skb)
\ENDIF
\ENDIF
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:non-ect-to-ect} changes non-ECT packets into ECT packets 
in the virtual switch before the packets enter the fabric and
change them back when they arrive at the destination virtual switch.
$p.pretendECT$ is set or cleared by flipping the 2nd bit of the 3 reserved bits in TCP header.
After applying this algorithm, all TCP packets traversing the network appear to be ECT packets. 
So the ECT and non-ECT coexistence issue is solved. 
Also, because we change non-ECT back at the destination virtual switch, 
so TCP is not aware of such a transformation and \acdc{} does not break any TCP connections or applications.
Algorithm~\ref{alg:non-ect-to-ect} leverages hardware checksumming feature in the NIC implicitly to reduce CPU overhead.
\todo{how to add comment in this algorithm?}.

In history, TCP used packet loss as the signal of network congestion. 
But, using packet loss as the signal can not meet the performance demand of datacenter environment 
due to high queueing latency and frequent packet losses. Packet loss-based congestion control 
can work reasonably well for large data transfers but does harm time-critical RPCs. 
Thus, new congestion control algorithms designed for datacenter networks were proposed. 
They can be classified into 2 categories: Explicit Congestion Notification (ECN)-based and 
latency-based. The most straightforward way is to treat ECN as packet loss and cut congestion 
window before network queue built up and packet loss happens---but this can harm flow throughput. 
DCTCP improves this by cut window properly based on the portion of marked packets and 
gives high throughput for large flows and low latency for mice flows. 
Latency-based congestion control such as TCP-NV, TIMELY, DX use the end-to-end latency 
as the signal of network congestion. Compared with ECN-based approach, latency is a more 
prevalent signal for congestion (i.e., does not need ECN capability on the switches). 
But the challenge of such approaches is to find ways to combat with latency measurement noise 
caused by TSO, LRO and interrupt coalescence.

This works main thesis is to explore whether we can provide universal low latency, low packet loss, 
high throughput and better fairness properties in the network virtualization layer or 
the network edge (e,g, NIC) which is completely under cloud provider's control such that 
we can enforce better network performance for public datacenters. 
%We leverage the existing mature congestion control algorithms and 
%will use DCTCP as an implementation example.

Connection tracking~\cite{ayuso2006netfilter,ovs-conntrack}.

TCP Receive Window Auto-Tuning~\cite{semke1998automatic,receive-auto-runing}.
Modern TCP stacks have this feature. In Linux and Window Server 2008, it is enabled by default.
Finding proper receive buffer sizing is difficult due to rapidly changing network conditions.

TCP receive window scaling~\cite{RFC1072,RFC1323}.

\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_15k.pdf}
                \caption{MTU = 1.5KB.}
                \label{effectiveness_15k}
        \end{subfigure}
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_9k.pdf}
                \caption{MTU = 9KB.}
                \label{effectiveness_9k}
        \end{subfigure}
        \caption{Using \rwnd{} can effectively control throughput. 
		Experiments are conducted on a 10G testbed. TCP CUBIC but New Reno shows similar results.
		Linux 3.18.0. We control maximal \rwnd{} value by modifying the receiver's advertised window size in TCP ACKs
		in the Open vSwitch. We control maximal \cwnd{} by specifying ``snd\_cwnd\_clamp" value in Linux TCP.}
        \label{rwnd_effectiveness}
\end{figure}
