\section{Design}
\label{design}
\begin{figure}[!t]
        \centering
  \includegraphics[width=0.45\textwidth]{figures/acdc_highlevel.pdf}
        \caption{\acdc{} high level view.}
        \label{acdc_highlevel}
\end{figure}

This section provides an overview of~\acdc{}'s design. First, we show how basic
congestion control state can be inferred in the vSwitch. Then we provide a case study
that details how to implement DCTCP. Finally, we discuss how we can enforce congestion
control in the vSwitch and provide a brief overview of how per-flow differentiation
can be easily implemented in our scheme.


\begin{figure}[th]
        \centering
  \includegraphics[width=0.45\textwidth]{figures/tcp-state.pdf}
        \caption{Variables for TCP.}
        \label{tcpstate}
\end{figure}
\subsection{Obtaining Congestion Control State}
\label{ss:tcpstate}
Figure~\ref{acdc_highlevel} shows the high level structure of \acdc{}. Since it is
implemented in the vSwitch, all traffic can be monitored which allows us to reconstruct
the state needed for congestion control. And because~\acdc{} is run on all nodes, per-flow
tunnels can be established to alter traffic and maintain per-flow state.

We first demonstrate how congestion control state can be easily inferred.
Figure~\ref{tcpstate} provides a visual of the sequence number space. The {\tt snd\_una} variable is the first byte
that has been sent, but not yet ACKed. The {\tt snd\_nxt} variable is the 
next byte to be sent. Bytes between {\tt snd\_una} and {\tt snd\_nxt} are in flight.
The largest number of packets that can be sent and unacknowledged is bounded by \cwnd{}.
{\tt snd\_una} is simple to update: each ACK contains an acknowledgement number ({\tt tcp.ack\_seq}), and 
we update {\tt snd\_una} when {\tt tcp.ack\_seq} $>$ {\tt snd\_una}.
When packets traverse the vSwitch from the VM, {\tt snd\_nxt} is updated if the sequence
number is larger than the current {\tt snd\_nxt} value.
Detecting loss is also relatively simple. If {\tt tcp.ack\_seq $\le$ snd\_una}, then
a local {\tt dupack} counter is updated. Timeouts can be inferred when {\tt snd\_una $<$ snd\_nxt}
and an inactivity timer fires. The slow start threshold ({\tt ssthresh}) is set to
a default value~\eric{keqiang, what is it?} and then is updated as specified by the congestion control algorithm.
By obtaining this state, the vSwitch can determine appropriate~\cwnd{} values for canonical
TCP congestion control schemes.
More advanced congestion control techniques that rely on timestamps, round-trip times, SACKs or 
FACKs can also be implemented. We omit the details in the interest of space. 


\subsection{Implementing DCTCP}
%\begin{figure}[!t]
%        \centering
%  \includegraphics[width=0.45\textwidth]{figures/acdc_ECT_marking.pdf}
%        \caption{Simple stateless ECT marking in \acdc{}. 
%		Turn all intra-DC TCP packets in the fabric into ECT.}
%        \label{acdc_ect_marking}
%\end{figure}

%\begin{figure}[!t]
%        \centering
%  \includegraphics[width=0.25\textwidth]{figures/carry_ecn_info.pdf}
%        \caption{Carrying the congestion information back. 
%		Being compatible with TSO and minimizing CPU and traffic overhead.}
%        \label{acdc_carry_ecn_info}
%\end{figure}

\begin{figure}[!t]
        \centering
  \includegraphics[width=0.45\textwidth]{figures/acdc_cc.pdf}
        \caption{DCTCP-like congestion control in \acdc{}.}
        \label{acdc_cc}
\end{figure}

This section discusses how to obtain additional state needed to perform DCTCP's congestion control
and then provides psuedocode for the actual derivation.


\tightparagraph{ECN Marking.} 
DCTCP requires flows to be ECN capable, but the VM's TCP stack may not support ECN.
Thus, all egress packets are marked to be ECN capable on the sender module. 
These packets traverse the network and the receiver module detects if they are marked.
When the VM TCP stack does not support ECN, all ECN-related bits are stripped at the sender
and receiver module in order to preserve the original TCP settings. When the VM TCP
stack does support ECN, the~\acdc{} receiver module strips the {\tt congestion encountered} 
bits from packets in order to prevent the VM TCP stack from decreasing rates too aggressively
(recall DCTCP adjusts~\cwnd{} proportionally to the fraction of packets
that encounter congestion, while traditional schemes conservatively reduce~\cwnd{} by half). A bit
in the reserved field of the header is used to determine if the VM TCP stack originally 
supported ECN.
~\eric{we need to have some discussion about how we might be able to make some flows 
send faster if they are using our scheme. this is because: 1. we hide ECN info
from host TCP stacks and 2. DCTCP mitigates loss for the most part, so the host
TCP stack never has a reason to back off-- they are always limited by RWND.}
%Algorithm~\ref{alg:non-ect-to-ect} changes non-ECT packets into ECT packets
%in the virtual switch before the packets enter the fabric and
%change them back when they arrive at the destination virtual switch.
%$p.pretendECT$ is set or cleared by flipping the 2nd bit of the 3 reserved bits in TCP header.
%After applying this algorithm, all TCP packets traversing the network appear to be ECT packets.
%So the ECT and non-ECT coexistence issue is solved.
%Also, because we change non-ECT back at the destination virtual switch,
%so TCP is not aware of such a transformation and \acdc{} does not break any TCP connections or applications.
%Algorithm~\ref{alg:non-ect-to-ect} leverages hardware checksumming feature in the NIC implicitly to reduce CPU overhead.
%\todo{how to add comment in this algorithm?}.
%
%
%\begin{algorithm}[t]
%\caption{ECT marking in \acdc{}}
%\label{alg:non-ect-to-ect}
%\begin{algorithmic}[1]
%%\STATE \Comment{packet $p$ is TCP packet}
%\IF{$p$ is going to the network}
%\IF{$p$ is non-ECT}
%\STATE change $p$ to ECT by modifying IP header
%\STATE set $p.pretendECT$
%%\STATE set the 2nd bit of the 3 reserved bits in TCP header to 1
%\ENDIF
%\ELSIF{$p$ is coming from the network}
%\IF{$p$ is non-ECT and $p.pretendECT$ is set}
%\STATE change $p$ back to non-ECT by modifying IP header
%\STATE clear $p.pretendECT$
%%\STATE clear the 2nd bit of the reserved bits
%\ENDIF
%\ENDIF
%\end{algorithmic}
%\end{algorithm}
%

\tightparagraph{Obtaining ECN feedback.}
In DCTCP, the fraction of packets experiencing congestion needs to be reported to the sender. 
%DCTCP relays this information from the destination to the sender by essentially modulating data
%in the ACK reporting scheme. Sender and receiver agree on an ACK state-machine that effectively 
%communicates how many packets were received with ECN vs how many were not. Because the TCP stack
%in the VM may not be DCTCP, we do not want to rely on the ACK modulation scheme. 
Since the VM TCP stack may not support DCTCP, the~\acdc{} receiver module monitors
the total bytes and ECN-marked bytes received on a per-flow basis. Receivers then report 
this information to the sender. Our scheme simply piggy-backs the
reported totals on ACKs from the receiver. When piggy-backing
does not violate the MTU size, it is inserted as a TCP Option. This is called 
a Piggy-backed ACK (PACK). The PACK is created by moving the IP and TCP headers into
the ACK packet's {\tt skb} headroom. The PACK is inserted into the vacated space and the memory
consumed by the rest of the packet is left as is. The IP checksum, IP packet length and TCP Data Offset fields~\eric{Keqiang, check!} are changed and the 
TCP checksum is calculated by the NIC. The PACK is stripped at the
sender so it is not exposed to the VM's TCP stack. The PACK size is only 8 bytes.

If adding a PACK violates the MTU size, instead a dedicated packet 
called a Fake ACK (FACK) is sent. This is sent in addition to the real TCP ACK (RACK).
FACKs are also discarded by the sender after logging the included
data. In practice, we find that most feedback takes the form of PACKs and RACKs/FACKs are rarely
sent. 

%
%\begin{algorithm}[t]
%\caption{Carrying the congestion information back}
%\label{alg:outgoing}
%\begin{algorithmic}[1]
%%\STATE flow\_key $\leftarrow$ \{dstip, srcip, dstport, srcport\}
%%\STATE ack\_entry $\leftarrow$ rcv\_ack\_hashtbl\_lookup(flow\_key)
%%\STATE end\_seq $\leftarrow$ ntohl(tcp.seq) + tcp.data\_len
%%\IF{after(end\_seq, ack\_entry.snd\_nxt)}
%%       \STATE ack\_entry.snd\_nxt $\leftarrow$ end\_seq
%%\ENDIF
%%\IF{tcp.ack}
%\IF{outgoing skb is a TCP ACK}
%\STATE flow\_key $\leftarrow$ \{dstip, srcip, dstport, srcport\}
%\STATE entry $\leftarrow$ rcv\_data\_hashtbl\_lookup(flow\_key)
%        \IF{entry.total\_bytes > 0}
%                \STATE ecn\_bytes $\leftarrow$ entry.ecn\_bytes
%                \STATE total\_bytes $\leftarrow$ entry.total\_bytes
%                \STATE entry.ecn\_bytes $\leftarrow$ 0
%                \STATE entry.total\_bytes $\leftarrow$ 0
%        \ENDIF
%        \IF{total\_bytes > 0}
%                \IF{skb.data\_len < (MTU -- HEADROOM)}
%                        \STATE skb $\leftarrow$ ovs\_pack(skb, ecn\_bytes, total\_bytes)
%                \ELSE
%                        \STATE fack $\leftarrow$ ovs\_fack(skb, ecn\_bytes, total\_bytes)
%                \ENDIF
%        \ENDIF
%\ENDIF
%\IF{fack != NULL}
%        \STATE sendtoNIC(fack)
%\ENDIF
%\STATE sendtoNIC(skb)
%\end{algorithmic}
%\end{algorithm}
%


\tightparagraph{DCTCP congestion control.} 
Once the fraction of ECN-marked packets is obtained, implementing DCTCP's logic becomes straight-forward.
Figure~\ref{acdc_cc} shows the high-level design. 
First, congestion control (CC) information is extracted from FACKs and PACKs. Connection tracking
variables (those described in Section~\ref{ss:tcpstate}) are updated based on the ACK. The variable
$\alpha$ is an EWMA of the fraction of packets that experienced congestion, and is updated roughly
once per RTT. If congestion was not encountered (no loss and no ECN), then {\tt tcp\_cong\_avoid} advances the congestion window
based on TCP New Reno's algorithm, using slow start or congestion avoidence as needed. If congestion was
experienced, then the congestion window must be reduced. DCTCP's instructions indicate the window should
only be cut once per RTT.  While this discussion is mostly high-level, additional details 
can easily be found in the
DCTCP paper~\cite{alizadeh2011data}, its RFC~\eric{cite} and its associated Linux source code~\eric{add judd here?}.
Our implementation closely tracks the Linux source code.
Once the congestion window is determined,~\acdc{} must enforce that value on
the VM's TCP flow, which will be described in the next subsection.


%\begin{algorithm}[!tbh]
%\caption{Congestion control in \acdc{}}
%\label{alg:incoming}
%\begin{algorithmic}[1]
%\IF{incoming skb is a TCP ACK}
%\STATE initialize is\_rack, is\_pack and is\_fack
%\STATE flow\_key $\leftarrow$ \{srcip, dstip, srcport, dstport\}
%\STATE entry $\leftarrow$ rcv\_ack\_hashtbl\_lookup(flow\_key)
%\IF{is\_pack or is\_fack}
%        \STATE unpack and extract ecn\_bytes and total\_bytes
%        \STATE update(entry.ecn\_bytes, ecn\_bytes)
%        \STATE update(entry.total\_bytes, total\_bytes)
%\ENDIF
%\IF{is\_rack or is\_pack}
%        \STATE acked $\leftarrow$ tcp.ack\_seq -- entry.snd\_una
%        \STATE entry.snd\_una $\leftarrow$ tcp.ack\_seq
%        \IF{duplicated ACK}
%                \STATE entry.dupack\_cnt++
%        \ENDIF
%        \STATE dctcp\_update\_alpha(entry)
%        \IF{acked > 0}
%                \IF{may\_raise\_rwnd(entry)}
%                        \STATE tcp\_cong\_avoid(entry, acked)
%                \ENDIF
%        \ENDIF
%\ENDIF
%\IF{entry.dupack\_cnt >= 3 or ecn\_bytes > 0}
%        \IF{entry.dupack\_cnt >= 3}
%                \STATE entry.alpha $\leftarrow$ MAX\_ALPHA
%                \STATE entry.loss $\leftarrow$ true
%        \ENDIF
%        \IF{may\_reduce\_rwnd(entry)}
%                \STATE entry.ssthresh $\leftarrow$ dctcp\_ssthresh(ack\_entry)
%                \STATE entry.rwnd $\leftarrow$ min(RWND\_MIN, entry.ssthresh)
%                \STATE entry.reduced $\leftarrow$ true
%        \ENDIF
%\ENDIF
%\IF{is\_rack or is\_pack}
%        \STATE tcp.window $\leftarrow$ min(tcp.window, entry.rwnd)
%\ENDIF
%\IF{is\_fack}
%        \STATE drop(skb)
%\ENDIF
%\ENDIF
%\end{algorithmic}
%\end{algorithm}
%

\begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_15k.pdf}
                \caption{MTU = 1.5kB.}
                \label{effectiveness_15k}
        \end{subfigure}
        \begin{subfigure}[b]{0.225\textwidth}
                \centering
                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_9k.pdf}
                \caption{MTU = 9kB.}
                \label{effectiveness_9k}
        \end{subfigure}
        \caption{Using \rwnd{} can effectively control throughput.
                Experiments are conducted on a 10G testbed. TCP CUBIC but New Reno shows similar results.
                Linux 3.18.0. We control maximal \rwnd{} value by modifying the receiver's advertised window size in TCP ACKs
                in the Open vSwitch. We control maximal \cwnd{} by specifying ``snd\_cwnd\_clamp" value in Linux TCP.}
        \label{rwnd_effectiveness}
\end{figure}


\subsection{Enforcing congestion control}
There must be a mechanism to ensure the VM's TCP flow adheres to the congestion control window determined in the vSwitch.
Luckily, TCP provides built-in functionality that can easily be reprovisioned for~\acdc{}. Specifically,
TCP's flow control allows a receiver to advertise the amount of data it is willing to process from a sender via
a receive window (\rwnd{}).~\acdc{} reuses~\rwnd{}. Specifically, the vSwitch calculates its own congestion window and overwrites~\rwnd{} with
this value as long as it is smaller than the current~\rwnd{}.

Ensuring VM TCP flows adhere to the~\rwnd{} value is relatively simple. The vSwitch calculates a new congestion 
window every time an ACK is received. When the ACK is sent to the VM TCP flow, it should trigger 

 Rate-limiters are commonly used to admit VM traffic. They range from
software to hardware to hybrid schemes. Rather than enforcing rates, we work on windows.
We directly dictate how many packets a VM TCP flow can send by altering the~\rwnd{} value 
on incoming ACKs. TCP standard says that all flows must adhere to minimum of~\rwnd{} and~\cwnd{}.

VM's with unaltered TCP stacks will naturally follow our enforcement scheme because the stacks
will simply follow the standard. However, we need a policing scheme to ensure that VM TCP stacks
do not simply ignore the~\rwnd{} value. This is simple to do: we allow only~\rwnd{} outstanding 
packets at a time. Any packets that the VM tries to send violating this are dropped. Because
dropped packets have to be recovered end-to-end, this incentivizes users to respect the standard. 

\tightparagraph{Compatible with receive window auto-tuning and window scaling}
TCP receive window auto-tuning~\cite{semke1998automatic,receive-auto-runing} is a commonly used feature 
in modern TCP stacks. It is enabled by default in recent Linux and Windows distributions.
Receive window auto-tuning was proposed to automatically 
set socket buffer size based on network conditions to 
improve network performance while optimizing the memory usage by TCP connections.
Because \acdc{} overwrites the real \rwnd{} value embedded in TCP ACK header 
only when \acdc{}'s running \rwnd{} is smaller than 
the real \rwnd{}, thus \acdc{} does not break the correctness of TCP semantics 
and is compatible with receive window auto-tuning.

TCP receive window scaling was proposed to 
improve network throughput over high bandwidth delay product networks~\cite{RFC1072,RFC1323}. 
Historically, TCP receive window field is only 16-bit (65,535 bytes) and 
in high bandwidth delay production networks, this becomes a limiting factor for TCP throughput. 
To extend receive window size, a TCP window scaling factor was defined in TCP options. 
The true receive window size is the value in TCP receive window field 
left-shifted by the window scaling factor. 
TCP window scaling factor is negotiated between two end-points at the beginning of 
a TCP connection, so \acdc{} can interprets the flow's scaling factor 
by parsing the TCP option fields. 
If any of the two end-points does not support window scaling factor option, \acdc{} set it to 0 (\ie{}, no window scaling).




\subsection{Per-flow Differiatation}
\label{ss:cc-qos}
Note that multiple congestion control algorithms can be developed in OVS, just as in Linux.
Administrators can then assign flows to specific congestion control algorithms based on policy.
For example, flows destined to the WAN may be assigned CUBIC and flows destined within the
datacenter may be set to DCTCP.

Similarly, administrators can assign different priorities to flow throughputs by altering the 
congestion control algorithm. There are limited number of priority classes in switches, and 
an admin may want to differentiate data within a specific class. Freeium users is a good example.

\begin{equation}
rwnd = rwnd (1 - (\alpha - \frac{\alpha{}\beta}{2}))
\label{eqn:cc-qos}
\end{equation}

Differiantated CC inspired by~\cite{Venkataramani2002tcpnice,shieh2011sharing}.

\subsection{How to keep low overhead?}
Move to Implementation?
Connection tracking~\cite{ayuso2006netfilter,ovs-conntrack}.

\subsection{How is it complimentary to other schemes?}
Like bandwidth allocation.

~\eric{Should we talk about how we can actually increase rate of VM TCP?}



%In history, TCP used packet loss as the signal of network congestion. 
%But, using packet loss as the signal can not meet the performance demand of datacenter environment 
%due to high queueing latency and frequent packet losses. Packet loss-based congestion control 
%can work reasonably well for large data transfers but does harm time-critical RPCs. 
%Thus, new congestion control algorithms designed for datacenter networks were proposed. 
%They can be classified into 2 categories: Explicit Congestion Notification (ECN)-based and 
%latency-based. The most straightforward way is to treat ECN as packet loss and cut congestion 
%window before network queue built up and packet loss happens---but this can harm flow throughput. 
%DCTCP improves this by cut window properly based on the portion of marked packets and 
%gives high throughput for large flows and low latency for mice flows. 
%Latency-based congestion control such as TCP-NV, TIMELY, DX use the end-to-end latency 
%as the signal of network congestion. Compared with ECN-based approach, latency is a more 
%prevalent signal for congestion (i.e., does not need ECN capability on the switches). 
%But the challenge of such approaches is to find ways to combat with latency measurement noise 
%caused by TSO, LRO and interrupt coalescence.
%


%\begin{figure}[t]
%        \centering
%        \begin{subfigure}[b]{0.225\textwidth}
%                \centering
%                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_15k.pdf}
%                \caption{MTU = 1.5KB.}
%                \label{effectiveness_15k}
%        \end{subfigure}
%        \begin{subfigure}[b]{0.225\textwidth}
%                \centering
%                \includegraphics[width=\textwidth]{figures/new_enforce/new_tput_cwnd_rwnd_cubic_9k.pdf}
%                \caption{MTU = 9KB.}
%                \label{effectiveness_9k}
%        \end{subfigure}
%        \caption{Using \rwnd{} can effectively control throughput. 
%		Experiments are conducted on a 10G testbed. TCP CUBIC but New Reno shows similar results.
%		Linux 3.18.0. We control maximal \rwnd{} value by modifying the receiver's advertised window size in TCP ACKs
%		in the Open vSwitch. We control maximal \cwnd{} by specifying ``snd\_cwnd\_clamp" value in Linux TCP.}
%        \label{rwnd_effectiveness}
%\end{figure}
